{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G3aybgmdyqoM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"Temporary_data3_Left_Right_Copy.xlsx\")"
      ],
      "metadata": {
        "id": "pEbiH9coythM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "fbxcBUoj0a9G",
        "outputId": "0626ca0c-c227-44e3-fcd0-86a3786a559c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   SEX  EDUC  MARISTAT  INDEPEND  RESIDENC  NACCFAM  ANYMEDS  SMOKYRS  \\\n",
              "0    2    18         1         1         1        1        1        0   \n",
              "1    1    11         1         1         1        1        1        0   \n",
              "2    2    16         1         1         1        1        1       10   \n",
              "3    1    14         1         1         1        0        1        0   \n",
              "4    1    16         1         1         2        0        1       50   \n",
              "\n",
              "   NACCTBI  DIABETES  ALCOHOL  HXHYPER  HYPERCHO  HXSTROKE  FOCLSIGN  HACHIN  \\\n",
              "0        0         0        0        0         1         0         0       0   \n",
              "1        0         0        0        1         1         0         0       1   \n",
              "2        0         1        0        0         1         0         0       0   \n",
              "3        0         1        0        1         1         0         0       1   \n",
              "4        0         0        0        1         0         0         0       1   \n",
              "\n",
              "   CDRGLOB  DEL  HALL  AGIT  DEPD  ANX  ELAT  APA  DISN  IRR  MOT  NITE  APP  \\\n",
              "0        0    0     0     0     1    0     0    0     0    0    0     0    0   \n",
              "1        2    0     0     1     0    0     0    0     0    1    0     0    0   \n",
              "2        0    0     0     0     0    0     0    0     0    0    0     0    0   \n",
              "3        0    0     0     0     0    0     0    0     0    0    0     0    0   \n",
              "4        1    0     0     0     0    0     0    0     0    0    0     0    0   \n",
              "\n",
              "   NACCGDS  DROPACT  NACCMMSE  NACCAGEB  NACCAMD  NACCBMI  NACCAPOE  NACCNE4S  \\\n",
              "0        0        0        30        56       10     25.1         3         0   \n",
              "1        0        0        19        65        6     30.3         1         0   \n",
              "2        2        1        29        58        1     23.2         1         0   \n",
              "3        0        0        30        72        3     30.4         1         0   \n",
              "4        1        0        27        82        9     20.8         1         0   \n",
              "\n",
              "    NACCICV  NACCBRNV  NACCWMVL   CSFVOL  GRAYVOL  WHITEVOL  WMHVOL  CEREALL  \\\n",
              "0  1399.056  1118.929   481.802  279.718  637.536   481.393  0.4088  1189.23   \n",
              "1  1578.180  1107.680   477.910  468.740  631.530   476.150  1.7600  1382.83   \n",
              "2  1455.550  1187.760   527.470  266.410  661.670   526.090  1.3800  1247.03   \n",
              "3  1494.650  1115.330   478.960  376.840  638.850   476.480  2.4800  1248.03   \n",
              "4  1494.067  1014.743   444.578  471.128  578.361   436.382  8.1956  1281.46   \n",
              "\n",
              "   CERETISS  CERECSF   CEREGR   CEREWH  LHIPPO  RHIPPO  LLATVENT  RLATVENT  \\\n",
              "0   952.446  236.781  517.620  434.417  3.5363  3.4538    8.3738    6.8119   \n",
              "1   975.620  407.210  533.580  440.280  3.2000  2.6700   25.5100   56.3300   \n",
              "2  1035.400  211.630  551.930  482.100  3.1800  3.2600    4.0300    3.9000   \n",
              "3   965.820  282.210  535.920  427.430  3.5300  3.3800    9.0400   10.8300   \n",
              "4   885.615  395.844  486.349  391.074  2.8706  2.9756   38.7750   38.4131   \n",
              "\n",
              "   THIRVENT   LFRCORT   RFRCORT  LOCCORT  ROCCORT  LPARCORT  RPARCORT  \\\n",
              "0    0.8231   89.8988   90.9769  28.7775    30.66   51.9338   50.8688   \n",
              "1    2.0200   98.4800   93.6300  34.0800    33.01   46.1300   45.7200   \n",
              "2    0.7500  105.3500  103.0800  31.8500    29.44   52.3100   56.2200   \n",
              "3    1.1100   95.9200   99.6300  24.4200    31.95   51.1500   50.8600   \n",
              "4    1.6669   93.0694   91.7550  29.1394    27.87   38.4169   40.5619   \n",
              "\n",
              "   LTEMPCOR  RTEMPCOR    LCAC   LCACM    LCMF   LCMFM    LCUN   LCUNM    LENT  \\\n",
              "0   68.0513   62.5875  2.4731  1.9636  3.4238  1.7395  4.0425  1.2197  5.0494   \n",
              "1   63.6700   53.6600  3.0600  2.9800  5.4300  1.8600  5.3800  1.4000  3.9300   \n",
              "2   65.0700   65.4400  3.1900  2.3500  6.1200  1.8800  4.1600  1.1700  4.8800   \n",
              "3   71.1700   65.1600  3.6700  2.5500  4.1700  1.7200  3.0800  0.8700  4.8700   \n",
              "4   57.0300   59.4094  3.9075  2.4561  5.5219  2.1456  3.8925  0.9288  4.8000   \n",
              "\n",
              "    LENTM     LFUS   LFUSM  LINFPAR  LINFPARM  LINFTEMP  LINFTEMM  LINSULA  \\\n",
              "0  3.3557  10.7381  2.5392  12.3469    2.3224   12.5194    2.9848   6.1388   \n",
              "1  3.1700   9.3700  2.0700  11.0000    1.7700   10.0900    2.5300   6.5900   \n",
              "2  3.5300  10.2400  2.8200  11.7100    2.1400   10.7600    2.8600   6.5500   \n",
              "3  2.8800  10.5300  2.5100  11.6100    2.0300   11.8700    2.9900   6.6500   \n",
              "4  2.1765   8.9494  1.7894   9.4200    1.3756    9.4013    2.3596   5.2631   \n",
              "\n",
              "   LINSULAM  LISTHC  LISTHCM  LLATOCC  LLATOCCM  LLATORBF  LLATORBM   LLING  \\\n",
              "0     2.799  2.4263   2.2086  11.2106    1.8961    8.0156    2.4951  6.2644   \n",
              "1     3.410  3.2000   2.1100  11.6200    1.4200    8.7200    2.4100  7.2300   \n",
              "2     2.530  3.0800   2.4400  12.1400    2.0600    9.3800    2.5700  6.4800   \n",
              "3     2.840  3.1200   2.1900   8.5500    1.8900    9.9900    2.2100  6.0900   \n",
              "4     2.631  2.4394   1.4295  10.4813    1.2573    9.3356    2.2364  7.4419   \n",
              "\n",
              "   LLINGM  LMEDORBF  LMEDORBM  LMIDTEMP  LMIDTEMM  LPARCEN  LPARCENM  LPARHIP  \\\n",
              "0  1.3553    4.5619    2.2564   14.3644    2.7367   3.0975    1.2979   4.5094   \n",
              "1  1.2900    4.6600    2.4000   12.5200    1.8700   5.3500    2.0000   3.9000   \n",
              "2  1.5300    5.8800    2.6100   12.9400    2.6500   4.3700    1.4300   3.7800   \n",
              "3  1.2400    5.1400    2.1900   16.1700    2.3200   3.0700    1.0200   4.5900   \n",
              "4  0.9148    3.8363    1.7871   11.5200    2.1492   3.3281    0.9520   3.5325   \n",
              "\n",
              "   LPARHIPM  LPARSOP  LPARSOPM  LPARORB  LPARORBM  LPARTRI  LPARTRIM  LPERCAL  \\\n",
              "0    1.9378   5.1506    1.9780   1.9931    2.0589   5.1769    1.8350   2.1169   \n",
              "1    1.7600   5.9900    2.0900   1.5800    2.0700   4.2200    2.0100   3.3600   \n",
              "2    2.1300   4.4700    1.6700   2.3600    2.2900   4.2700    2.0500   2.4300   \n",
              "3    1.4800   4.4400    1.6800   2.1100    1.8200   5.3800    1.8400   1.5800   \n",
              "4    1.3397   4.8000    1.7196   2.0775    2.1237   3.9863    1.7105   1.5694   \n",
              "\n",
              "   LPERCALM  LPOSCEN  LPOSCENM  LPOSCIN  LPOSCINM  LPRECEN  LPRECENM  LPRECUN  \\\n",
              "0    1.2519   9.4013    1.2137   3.7763    1.8875  10.5131    1.4798  10.1063   \n",
              "1    1.1600   8.7900    1.5700   4.5800    2.1200  13.3400    1.9700   9.3700   \n",
              "2    1.1600   9.9200    1.3300   5.4100    2.2200  16.3500    1.6500  10.0300   \n",
              "3    0.7500  10.0200    1.2900   3.6800    2.1100  11.0900    1.3100  10.6900   \n",
              "4    0.8144   7.1269    1.1084   3.6975    1.5174  11.9944    1.4951   6.8494   \n",
              "\n",
              "   LPRECUNM  LROSANC  LROSANCM   LROSMF  LROSMFM   LSUPFR  LSUPFRM  LSUPPAR  \\\n",
              "0    1.8533   3.6656    2.8356  11.8181   2.1284  28.0594   2.2255  12.1406   \n",
              "1    1.7800   4.6600    3.3200  13.2200   1.9900  25.8600   2.5000  11.1700   \n",
              "2    1.7200   3.8000    2.9500  12.3400   2.7000  29.7100   2.5100  11.1800   \n",
              "3    1.6200   4.3100    2.5300  14.2700   2.3600  26.1900   2.3500  10.3200   \n",
              "4    0.9259   4.6744    2.4487  13.4606   2.1809  23.9400   1.9885   8.6888   \n",
              "\n",
              "   LSUPPARM  LSUPTEM  LSUPTEMM  LSUPMAR  LSUPMARM  LTRTEM  LTRTEMM    RCAC  \\\n",
              "0    1.4967  15.0938    1.9541  12.2400     1.884  0.9150   1.2724  1.6819   \n",
              "1    1.8400  18.1600    2.3500   9.2500     2.070  1.2800   2.2400  2.2700   \n",
              "2    1.5500  17.0500    2.0700  12.6400     1.740  1.0800   1.6200  0.8700   \n",
              "3    1.4700  17.0600    1.7600  12.0200     1.820  0.8200   1.0300  2.1100   \n",
              "4    1.0863  13.7925    1.6908   9.8719     1.728  0.8719   0.8916  2.5406   \n",
              "\n",
              "    RCACM    RCMF   RCMFM    RCUN   RCUNM    RENT   RENTM    RFUS   RFUSM  \\\n",
              "0  2.1905  7.2113  1.9629  4.0125  1.2922  4.0931  3.5253  9.3525  2.4511   \n",
              "1  3.4200  6.7600  1.9000  4.6800  1.5100  3.2900  2.3400  6.9300  1.5300   \n",
              "2  2.1300  8.2400  1.9600  4.6100  1.3200  4.4300  3.5100  9.3900  2.5200   \n",
              "3  2.5100  6.8200  1.8700  3.7300  1.1800  4.6800  3.2900  9.0800  2.2700   \n",
              "4  2.8680  8.2894  1.8736  3.9919  1.0694  3.5663  2.4244  7.3200  2.1698   \n",
              "\n",
              "   RINFPAR  RINFPARM  RINFTEMP  RINFTEMM  RINSULA  RINSULAM  RISTHC  RISTHCM  \\\n",
              "0  13.0313    2.3928   11.4619    3.3607   6.6225    3.2028  2.3231   2.2373   \n",
              "1  11.7300    1.9000   10.0800    2.4800   6.9200    3.6300  2.3600   1.6900   \n",
              "2  12.3800    2.2900   12.1600    2.8200   6.9100    2.9400  2.4700   2.3900   \n",
              "3  13.4000    1.9600   11.6800    2.8300   7.2800    2.6600  2.9700   2.2200   \n",
              "4  10.3575    1.4139   11.7469    2.5054   5.8650    2.9249  2.7675   1.6418   \n",
              "\n",
              "   RLATOCC  RLATOCCM  RLATORBF  RLATORBM   RLING  RLINGM  RMEDORBF  RMEDORBM  \\\n",
              "0  11.5238    2.0450    8.8388    2.5886  6.6544  1.3971    5.3531    2.3187   \n",
              "1  11.8800    1.3500    9.5300    2.5700  7.2600  1.1300    4.9900    2.4800   \n",
              "2   9.1800    2.3300   10.4100    2.7200  6.9500  1.5700    6.3700    2.5600   \n",
              "3  12.2200    1.6200    8.4700    2.3200  7.3800  1.2900    5.2300    2.3400   \n",
              "4   9.0488    1.3577    8.4000    1.9227  7.6931  1.1756    3.5831    1.8670   \n",
              "\n",
              "   RMIDTEMP  RMIDTEMM  RPARCEN  RPARCENM  RPARHIP  RPARHIPM  RPARSOP  \\\n",
              "0   15.3469    2.6912   3.2513    1.2745   4.2731    1.8069   4.7063   \n",
              "1    9.8800    1.9900   5.4500    2.1400   3.6400    1.3400   4.1100   \n",
              "2   13.6200    2.5000   5.0100    1.6700   4.4300    1.8400   4.5100   \n",
              "3   14.6800    2.2400   3.7500    1.1000   4.6600    1.8200   4.7600   \n",
              "4   13.0125    2.1627   3.6825    0.9625   4.1813    1.4396   4.0481   \n",
              "\n",
              "   RPARSOPM  RPARORB  RPARORBM  RPARTRI  RPARTRIM  RPERCAL  RPERCALM  RPOSCEN  \\\n",
              "0    1.9472   1.7475    1.9531   3.8381    1.8080   2.5706    1.3220  10.4494   \n",
              "1    2.0600   2.0200    2.2200   3.1400    1.7400   3.1100    1.2800   9.4400   \n",
              "2    1.7700   2.7800    2.1700   4.3700    1.9200   2.5500    1.6600  11.2600   \n",
              "3    1.6400   2.5100    2.0700   5.7500    1.9600   1.9900    0.8400   9.3500   \n",
              "4    1.3973   1.7513    1.6728   3.5325    1.6589   2.0156    0.9432   8.1525   \n",
              "\n",
              "   RPOSCENM  RPOSCIN  RPOSCINM  RPRECEN  RPRECENM  RPRECUN  RPRECUNM  RROSANC  \\\n",
              "0    1.4152   3.3844    2.0756  11.6831    1.8149   8.9925    1.8048   2.6363   \n",
              "1    1.6900   4.3500    2.6500  14.4500    2.0100   9.0000    1.7900   2.6400   \n",
              "2    1.2900   4.5800    2.2200  14.1200    1.7400   9.8200    1.8400   2.8200   \n",
              "3    1.1500   3.0800    1.9600  12.2000    1.3200   9.7900    1.7900   2.6200   \n",
              "4    1.1322   3.9956    1.8629  12.0750    1.1521   6.7988    1.3263   3.1856   \n",
              "\n",
              "   RROSANCM   RROSMF  RROSMFM   RSUPFR  RSUPFRM  RSUPPAR  RSUPPARM  RSUPTEM  \\\n",
              "0    2.9186  11.0550   2.0921  27.9038   2.2157  10.3481    1.7014  13.4250   \n",
              "1    3.0300   8.3800   1.7700  29.0600   2.6900   9.2500    1.6300  14.8800   \n",
              "2    3.0200   9.5000   2.5300  33.0000   2.5400  13.6900    1.5600  16.2300   \n",
              "3    2.5100  13.1600   2.5100  31.6800   2.4100   9.6400    1.4500  15.1100   \n",
              "4    2.2584  11.1506   1.9614  28.0631   1.9501   9.1125    1.0427  13.3763   \n",
              "\n",
              "   RSUPTEMM  RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  \n",
              "0    1.7419   9.8700    1.6792  0.8906   1.1947  \n",
              "1    2.3000   7.2700    1.8900  1.3800   2.0300  \n",
              "2    2.0700  10.1100    1.8400  1.0100   1.5000  \n",
              "3    1.6400  10.3900    1.8300  0.7400   1.1000  \n",
              "4    1.6270   7.5844    1.4595  0.6638   0.8527  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa180bbe-f179-40f9-882e-b96cb9af3266\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SEX</th>\n",
              "      <th>EDUC</th>\n",
              "      <th>MARISTAT</th>\n",
              "      <th>INDEPEND</th>\n",
              "      <th>RESIDENC</th>\n",
              "      <th>NACCFAM</th>\n",
              "      <th>ANYMEDS</th>\n",
              "      <th>SMOKYRS</th>\n",
              "      <th>NACCTBI</th>\n",
              "      <th>DIABETES</th>\n",
              "      <th>ALCOHOL</th>\n",
              "      <th>HXHYPER</th>\n",
              "      <th>HYPERCHO</th>\n",
              "      <th>HXSTROKE</th>\n",
              "      <th>FOCLSIGN</th>\n",
              "      <th>HACHIN</th>\n",
              "      <th>CDRGLOB</th>\n",
              "      <th>DEL</th>\n",
              "      <th>HALL</th>\n",
              "      <th>AGIT</th>\n",
              "      <th>DEPD</th>\n",
              "      <th>ANX</th>\n",
              "      <th>ELAT</th>\n",
              "      <th>APA</th>\n",
              "      <th>DISN</th>\n",
              "      <th>IRR</th>\n",
              "      <th>MOT</th>\n",
              "      <th>NITE</th>\n",
              "      <th>APP</th>\n",
              "      <th>NACCGDS</th>\n",
              "      <th>DROPACT</th>\n",
              "      <th>NACCMMSE</th>\n",
              "      <th>NACCAGEB</th>\n",
              "      <th>NACCAMD</th>\n",
              "      <th>NACCBMI</th>\n",
              "      <th>NACCAPOE</th>\n",
              "      <th>NACCNE4S</th>\n",
              "      <th>NACCICV</th>\n",
              "      <th>NACCBRNV</th>\n",
              "      <th>NACCWMVL</th>\n",
              "      <th>CSFVOL</th>\n",
              "      <th>GRAYVOL</th>\n",
              "      <th>WHITEVOL</th>\n",
              "      <th>WMHVOL</th>\n",
              "      <th>CEREALL</th>\n",
              "      <th>CERETISS</th>\n",
              "      <th>CERECSF</th>\n",
              "      <th>CEREGR</th>\n",
              "      <th>CEREWH</th>\n",
              "      <th>LHIPPO</th>\n",
              "      <th>RHIPPO</th>\n",
              "      <th>LLATVENT</th>\n",
              "      <th>RLATVENT</th>\n",
              "      <th>THIRVENT</th>\n",
              "      <th>LFRCORT</th>\n",
              "      <th>RFRCORT</th>\n",
              "      <th>LOCCORT</th>\n",
              "      <th>ROCCORT</th>\n",
              "      <th>LPARCORT</th>\n",
              "      <th>RPARCORT</th>\n",
              "      <th>LTEMPCOR</th>\n",
              "      <th>RTEMPCOR</th>\n",
              "      <th>LCAC</th>\n",
              "      <th>LCACM</th>\n",
              "      <th>LCMF</th>\n",
              "      <th>LCMFM</th>\n",
              "      <th>LCUN</th>\n",
              "      <th>LCUNM</th>\n",
              "      <th>LENT</th>\n",
              "      <th>LENTM</th>\n",
              "      <th>LFUS</th>\n",
              "      <th>LFUSM</th>\n",
              "      <th>LINFPAR</th>\n",
              "      <th>LINFPARM</th>\n",
              "      <th>LINFTEMP</th>\n",
              "      <th>LINFTEMM</th>\n",
              "      <th>LINSULA</th>\n",
              "      <th>LINSULAM</th>\n",
              "      <th>LISTHC</th>\n",
              "      <th>LISTHCM</th>\n",
              "      <th>LLATOCC</th>\n",
              "      <th>LLATOCCM</th>\n",
              "      <th>LLATORBF</th>\n",
              "      <th>LLATORBM</th>\n",
              "      <th>LLING</th>\n",
              "      <th>LLINGM</th>\n",
              "      <th>LMEDORBF</th>\n",
              "      <th>LMEDORBM</th>\n",
              "      <th>LMIDTEMP</th>\n",
              "      <th>LMIDTEMM</th>\n",
              "      <th>LPARCEN</th>\n",
              "      <th>LPARCENM</th>\n",
              "      <th>LPARHIP</th>\n",
              "      <th>LPARHIPM</th>\n",
              "      <th>LPARSOP</th>\n",
              "      <th>LPARSOPM</th>\n",
              "      <th>LPARORB</th>\n",
              "      <th>LPARORBM</th>\n",
              "      <th>LPARTRI</th>\n",
              "      <th>LPARTRIM</th>\n",
              "      <th>LPERCAL</th>\n",
              "      <th>LPERCALM</th>\n",
              "      <th>LPOSCEN</th>\n",
              "      <th>LPOSCENM</th>\n",
              "      <th>LPOSCIN</th>\n",
              "      <th>LPOSCINM</th>\n",
              "      <th>LPRECEN</th>\n",
              "      <th>LPRECENM</th>\n",
              "      <th>LPRECUN</th>\n",
              "      <th>LPRECUNM</th>\n",
              "      <th>LROSANC</th>\n",
              "      <th>LROSANCM</th>\n",
              "      <th>LROSMF</th>\n",
              "      <th>LROSMFM</th>\n",
              "      <th>LSUPFR</th>\n",
              "      <th>LSUPFRM</th>\n",
              "      <th>LSUPPAR</th>\n",
              "      <th>LSUPPARM</th>\n",
              "      <th>LSUPTEM</th>\n",
              "      <th>LSUPTEMM</th>\n",
              "      <th>LSUPMAR</th>\n",
              "      <th>LSUPMARM</th>\n",
              "      <th>LTRTEM</th>\n",
              "      <th>LTRTEMM</th>\n",
              "      <th>RCAC</th>\n",
              "      <th>RCACM</th>\n",
              "      <th>RCMF</th>\n",
              "      <th>RCMFM</th>\n",
              "      <th>RCUN</th>\n",
              "      <th>RCUNM</th>\n",
              "      <th>RENT</th>\n",
              "      <th>RENTM</th>\n",
              "      <th>RFUS</th>\n",
              "      <th>RFUSM</th>\n",
              "      <th>RINFPAR</th>\n",
              "      <th>RINFPARM</th>\n",
              "      <th>RINFTEMP</th>\n",
              "      <th>RINFTEMM</th>\n",
              "      <th>RINSULA</th>\n",
              "      <th>RINSULAM</th>\n",
              "      <th>RISTHC</th>\n",
              "      <th>RISTHCM</th>\n",
              "      <th>RLATOCC</th>\n",
              "      <th>RLATOCCM</th>\n",
              "      <th>RLATORBF</th>\n",
              "      <th>RLATORBM</th>\n",
              "      <th>RLING</th>\n",
              "      <th>RLINGM</th>\n",
              "      <th>RMEDORBF</th>\n",
              "      <th>RMEDORBM</th>\n",
              "      <th>RMIDTEMP</th>\n",
              "      <th>RMIDTEMM</th>\n",
              "      <th>RPARCEN</th>\n",
              "      <th>RPARCENM</th>\n",
              "      <th>RPARHIP</th>\n",
              "      <th>RPARHIPM</th>\n",
              "      <th>RPARSOP</th>\n",
              "      <th>RPARSOPM</th>\n",
              "      <th>RPARORB</th>\n",
              "      <th>RPARORBM</th>\n",
              "      <th>RPARTRI</th>\n",
              "      <th>RPARTRIM</th>\n",
              "      <th>RPERCAL</th>\n",
              "      <th>RPERCALM</th>\n",
              "      <th>RPOSCEN</th>\n",
              "      <th>RPOSCENM</th>\n",
              "      <th>RPOSCIN</th>\n",
              "      <th>RPOSCINM</th>\n",
              "      <th>RPRECEN</th>\n",
              "      <th>RPRECENM</th>\n",
              "      <th>RPRECUN</th>\n",
              "      <th>RPRECUNM</th>\n",
              "      <th>RROSANC</th>\n",
              "      <th>RROSANCM</th>\n",
              "      <th>RROSMF</th>\n",
              "      <th>RROSMFM</th>\n",
              "      <th>RSUPFR</th>\n",
              "      <th>RSUPFRM</th>\n",
              "      <th>RSUPPAR</th>\n",
              "      <th>RSUPPARM</th>\n",
              "      <th>RSUPTEM</th>\n",
              "      <th>RSUPTEMM</th>\n",
              "      <th>RSUPMAR</th>\n",
              "      <th>RSUPMARM</th>\n",
              "      <th>RTRTEM</th>\n",
              "      <th>RTRTEMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>56</td>\n",
              "      <td>10</td>\n",
              "      <td>25.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1399.056</td>\n",
              "      <td>1118.929</td>\n",
              "      <td>481.802</td>\n",
              "      <td>279.718</td>\n",
              "      <td>637.536</td>\n",
              "      <td>481.393</td>\n",
              "      <td>0.4088</td>\n",
              "      <td>1189.23</td>\n",
              "      <td>952.446</td>\n",
              "      <td>236.781</td>\n",
              "      <td>517.620</td>\n",
              "      <td>434.417</td>\n",
              "      <td>3.5363</td>\n",
              "      <td>3.4538</td>\n",
              "      <td>8.3738</td>\n",
              "      <td>6.8119</td>\n",
              "      <td>0.8231</td>\n",
              "      <td>89.8988</td>\n",
              "      <td>90.9769</td>\n",
              "      <td>28.7775</td>\n",
              "      <td>30.66</td>\n",
              "      <td>51.9338</td>\n",
              "      <td>50.8688</td>\n",
              "      <td>68.0513</td>\n",
              "      <td>62.5875</td>\n",
              "      <td>2.4731</td>\n",
              "      <td>1.9636</td>\n",
              "      <td>3.4238</td>\n",
              "      <td>1.7395</td>\n",
              "      <td>4.0425</td>\n",
              "      <td>1.2197</td>\n",
              "      <td>5.0494</td>\n",
              "      <td>3.3557</td>\n",
              "      <td>10.7381</td>\n",
              "      <td>2.5392</td>\n",
              "      <td>12.3469</td>\n",
              "      <td>2.3224</td>\n",
              "      <td>12.5194</td>\n",
              "      <td>2.9848</td>\n",
              "      <td>6.1388</td>\n",
              "      <td>2.799</td>\n",
              "      <td>2.4263</td>\n",
              "      <td>2.2086</td>\n",
              "      <td>11.2106</td>\n",
              "      <td>1.8961</td>\n",
              "      <td>8.0156</td>\n",
              "      <td>2.4951</td>\n",
              "      <td>6.2644</td>\n",
              "      <td>1.3553</td>\n",
              "      <td>4.5619</td>\n",
              "      <td>2.2564</td>\n",
              "      <td>14.3644</td>\n",
              "      <td>2.7367</td>\n",
              "      <td>3.0975</td>\n",
              "      <td>1.2979</td>\n",
              "      <td>4.5094</td>\n",
              "      <td>1.9378</td>\n",
              "      <td>5.1506</td>\n",
              "      <td>1.9780</td>\n",
              "      <td>1.9931</td>\n",
              "      <td>2.0589</td>\n",
              "      <td>5.1769</td>\n",
              "      <td>1.8350</td>\n",
              "      <td>2.1169</td>\n",
              "      <td>1.2519</td>\n",
              "      <td>9.4013</td>\n",
              "      <td>1.2137</td>\n",
              "      <td>3.7763</td>\n",
              "      <td>1.8875</td>\n",
              "      <td>10.5131</td>\n",
              "      <td>1.4798</td>\n",
              "      <td>10.1063</td>\n",
              "      <td>1.8533</td>\n",
              "      <td>3.6656</td>\n",
              "      <td>2.8356</td>\n",
              "      <td>11.8181</td>\n",
              "      <td>2.1284</td>\n",
              "      <td>28.0594</td>\n",
              "      <td>2.2255</td>\n",
              "      <td>12.1406</td>\n",
              "      <td>1.4967</td>\n",
              "      <td>15.0938</td>\n",
              "      <td>1.9541</td>\n",
              "      <td>12.2400</td>\n",
              "      <td>1.884</td>\n",
              "      <td>0.9150</td>\n",
              "      <td>1.2724</td>\n",
              "      <td>1.6819</td>\n",
              "      <td>2.1905</td>\n",
              "      <td>7.2113</td>\n",
              "      <td>1.9629</td>\n",
              "      <td>4.0125</td>\n",
              "      <td>1.2922</td>\n",
              "      <td>4.0931</td>\n",
              "      <td>3.5253</td>\n",
              "      <td>9.3525</td>\n",
              "      <td>2.4511</td>\n",
              "      <td>13.0313</td>\n",
              "      <td>2.3928</td>\n",
              "      <td>11.4619</td>\n",
              "      <td>3.3607</td>\n",
              "      <td>6.6225</td>\n",
              "      <td>3.2028</td>\n",
              "      <td>2.3231</td>\n",
              "      <td>2.2373</td>\n",
              "      <td>11.5238</td>\n",
              "      <td>2.0450</td>\n",
              "      <td>8.8388</td>\n",
              "      <td>2.5886</td>\n",
              "      <td>6.6544</td>\n",
              "      <td>1.3971</td>\n",
              "      <td>5.3531</td>\n",
              "      <td>2.3187</td>\n",
              "      <td>15.3469</td>\n",
              "      <td>2.6912</td>\n",
              "      <td>3.2513</td>\n",
              "      <td>1.2745</td>\n",
              "      <td>4.2731</td>\n",
              "      <td>1.8069</td>\n",
              "      <td>4.7063</td>\n",
              "      <td>1.9472</td>\n",
              "      <td>1.7475</td>\n",
              "      <td>1.9531</td>\n",
              "      <td>3.8381</td>\n",
              "      <td>1.8080</td>\n",
              "      <td>2.5706</td>\n",
              "      <td>1.3220</td>\n",
              "      <td>10.4494</td>\n",
              "      <td>1.4152</td>\n",
              "      <td>3.3844</td>\n",
              "      <td>2.0756</td>\n",
              "      <td>11.6831</td>\n",
              "      <td>1.8149</td>\n",
              "      <td>8.9925</td>\n",
              "      <td>1.8048</td>\n",
              "      <td>2.6363</td>\n",
              "      <td>2.9186</td>\n",
              "      <td>11.0550</td>\n",
              "      <td>2.0921</td>\n",
              "      <td>27.9038</td>\n",
              "      <td>2.2157</td>\n",
              "      <td>10.3481</td>\n",
              "      <td>1.7014</td>\n",
              "      <td>13.4250</td>\n",
              "      <td>1.7419</td>\n",
              "      <td>9.8700</td>\n",
              "      <td>1.6792</td>\n",
              "      <td>0.8906</td>\n",
              "      <td>1.1947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>65</td>\n",
              "      <td>6</td>\n",
              "      <td>30.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1578.180</td>\n",
              "      <td>1107.680</td>\n",
              "      <td>477.910</td>\n",
              "      <td>468.740</td>\n",
              "      <td>631.530</td>\n",
              "      <td>476.150</td>\n",
              "      <td>1.7600</td>\n",
              "      <td>1382.83</td>\n",
              "      <td>975.620</td>\n",
              "      <td>407.210</td>\n",
              "      <td>533.580</td>\n",
              "      <td>440.280</td>\n",
              "      <td>3.2000</td>\n",
              "      <td>2.6700</td>\n",
              "      <td>25.5100</td>\n",
              "      <td>56.3300</td>\n",
              "      <td>2.0200</td>\n",
              "      <td>98.4800</td>\n",
              "      <td>93.6300</td>\n",
              "      <td>34.0800</td>\n",
              "      <td>33.01</td>\n",
              "      <td>46.1300</td>\n",
              "      <td>45.7200</td>\n",
              "      <td>63.6700</td>\n",
              "      <td>53.6600</td>\n",
              "      <td>3.0600</td>\n",
              "      <td>2.9800</td>\n",
              "      <td>5.4300</td>\n",
              "      <td>1.8600</td>\n",
              "      <td>5.3800</td>\n",
              "      <td>1.4000</td>\n",
              "      <td>3.9300</td>\n",
              "      <td>3.1700</td>\n",
              "      <td>9.3700</td>\n",
              "      <td>2.0700</td>\n",
              "      <td>11.0000</td>\n",
              "      <td>1.7700</td>\n",
              "      <td>10.0900</td>\n",
              "      <td>2.5300</td>\n",
              "      <td>6.5900</td>\n",
              "      <td>3.410</td>\n",
              "      <td>3.2000</td>\n",
              "      <td>2.1100</td>\n",
              "      <td>11.6200</td>\n",
              "      <td>1.4200</td>\n",
              "      <td>8.7200</td>\n",
              "      <td>2.4100</td>\n",
              "      <td>7.2300</td>\n",
              "      <td>1.2900</td>\n",
              "      <td>4.6600</td>\n",
              "      <td>2.4000</td>\n",
              "      <td>12.5200</td>\n",
              "      <td>1.8700</td>\n",
              "      <td>5.3500</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>3.9000</td>\n",
              "      <td>1.7600</td>\n",
              "      <td>5.9900</td>\n",
              "      <td>2.0900</td>\n",
              "      <td>1.5800</td>\n",
              "      <td>2.0700</td>\n",
              "      <td>4.2200</td>\n",
              "      <td>2.0100</td>\n",
              "      <td>3.3600</td>\n",
              "      <td>1.1600</td>\n",
              "      <td>8.7900</td>\n",
              "      <td>1.5700</td>\n",
              "      <td>4.5800</td>\n",
              "      <td>2.1200</td>\n",
              "      <td>13.3400</td>\n",
              "      <td>1.9700</td>\n",
              "      <td>9.3700</td>\n",
              "      <td>1.7800</td>\n",
              "      <td>4.6600</td>\n",
              "      <td>3.3200</td>\n",
              "      <td>13.2200</td>\n",
              "      <td>1.9900</td>\n",
              "      <td>25.8600</td>\n",
              "      <td>2.5000</td>\n",
              "      <td>11.1700</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>18.1600</td>\n",
              "      <td>2.3500</td>\n",
              "      <td>9.2500</td>\n",
              "      <td>2.070</td>\n",
              "      <td>1.2800</td>\n",
              "      <td>2.2400</td>\n",
              "      <td>2.2700</td>\n",
              "      <td>3.4200</td>\n",
              "      <td>6.7600</td>\n",
              "      <td>1.9000</td>\n",
              "      <td>4.6800</td>\n",
              "      <td>1.5100</td>\n",
              "      <td>3.2900</td>\n",
              "      <td>2.3400</td>\n",
              "      <td>6.9300</td>\n",
              "      <td>1.5300</td>\n",
              "      <td>11.7300</td>\n",
              "      <td>1.9000</td>\n",
              "      <td>10.0800</td>\n",
              "      <td>2.4800</td>\n",
              "      <td>6.9200</td>\n",
              "      <td>3.6300</td>\n",
              "      <td>2.3600</td>\n",
              "      <td>1.6900</td>\n",
              "      <td>11.8800</td>\n",
              "      <td>1.3500</td>\n",
              "      <td>9.5300</td>\n",
              "      <td>2.5700</td>\n",
              "      <td>7.2600</td>\n",
              "      <td>1.1300</td>\n",
              "      <td>4.9900</td>\n",
              "      <td>2.4800</td>\n",
              "      <td>9.8800</td>\n",
              "      <td>1.9900</td>\n",
              "      <td>5.4500</td>\n",
              "      <td>2.1400</td>\n",
              "      <td>3.6400</td>\n",
              "      <td>1.3400</td>\n",
              "      <td>4.1100</td>\n",
              "      <td>2.0600</td>\n",
              "      <td>2.0200</td>\n",
              "      <td>2.2200</td>\n",
              "      <td>3.1400</td>\n",
              "      <td>1.7400</td>\n",
              "      <td>3.1100</td>\n",
              "      <td>1.2800</td>\n",
              "      <td>9.4400</td>\n",
              "      <td>1.6900</td>\n",
              "      <td>4.3500</td>\n",
              "      <td>2.6500</td>\n",
              "      <td>14.4500</td>\n",
              "      <td>2.0100</td>\n",
              "      <td>9.0000</td>\n",
              "      <td>1.7900</td>\n",
              "      <td>2.6400</td>\n",
              "      <td>3.0300</td>\n",
              "      <td>8.3800</td>\n",
              "      <td>1.7700</td>\n",
              "      <td>29.0600</td>\n",
              "      <td>2.6900</td>\n",
              "      <td>9.2500</td>\n",
              "      <td>1.6300</td>\n",
              "      <td>14.8800</td>\n",
              "      <td>2.3000</td>\n",
              "      <td>7.2700</td>\n",
              "      <td>1.8900</td>\n",
              "      <td>1.3800</td>\n",
              "      <td>2.0300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>23.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1455.550</td>\n",
              "      <td>1187.760</td>\n",
              "      <td>527.470</td>\n",
              "      <td>266.410</td>\n",
              "      <td>661.670</td>\n",
              "      <td>526.090</td>\n",
              "      <td>1.3800</td>\n",
              "      <td>1247.03</td>\n",
              "      <td>1035.400</td>\n",
              "      <td>211.630</td>\n",
              "      <td>551.930</td>\n",
              "      <td>482.100</td>\n",
              "      <td>3.1800</td>\n",
              "      <td>3.2600</td>\n",
              "      <td>4.0300</td>\n",
              "      <td>3.9000</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>105.3500</td>\n",
              "      <td>103.0800</td>\n",
              "      <td>31.8500</td>\n",
              "      <td>29.44</td>\n",
              "      <td>52.3100</td>\n",
              "      <td>56.2200</td>\n",
              "      <td>65.0700</td>\n",
              "      <td>65.4400</td>\n",
              "      <td>3.1900</td>\n",
              "      <td>2.3500</td>\n",
              "      <td>6.1200</td>\n",
              "      <td>1.8800</td>\n",
              "      <td>4.1600</td>\n",
              "      <td>1.1700</td>\n",
              "      <td>4.8800</td>\n",
              "      <td>3.5300</td>\n",
              "      <td>10.2400</td>\n",
              "      <td>2.8200</td>\n",
              "      <td>11.7100</td>\n",
              "      <td>2.1400</td>\n",
              "      <td>10.7600</td>\n",
              "      <td>2.8600</td>\n",
              "      <td>6.5500</td>\n",
              "      <td>2.530</td>\n",
              "      <td>3.0800</td>\n",
              "      <td>2.4400</td>\n",
              "      <td>12.1400</td>\n",
              "      <td>2.0600</td>\n",
              "      <td>9.3800</td>\n",
              "      <td>2.5700</td>\n",
              "      <td>6.4800</td>\n",
              "      <td>1.5300</td>\n",
              "      <td>5.8800</td>\n",
              "      <td>2.6100</td>\n",
              "      <td>12.9400</td>\n",
              "      <td>2.6500</td>\n",
              "      <td>4.3700</td>\n",
              "      <td>1.4300</td>\n",
              "      <td>3.7800</td>\n",
              "      <td>2.1300</td>\n",
              "      <td>4.4700</td>\n",
              "      <td>1.6700</td>\n",
              "      <td>2.3600</td>\n",
              "      <td>2.2900</td>\n",
              "      <td>4.2700</td>\n",
              "      <td>2.0500</td>\n",
              "      <td>2.4300</td>\n",
              "      <td>1.1600</td>\n",
              "      <td>9.9200</td>\n",
              "      <td>1.3300</td>\n",
              "      <td>5.4100</td>\n",
              "      <td>2.2200</td>\n",
              "      <td>16.3500</td>\n",
              "      <td>1.6500</td>\n",
              "      <td>10.0300</td>\n",
              "      <td>1.7200</td>\n",
              "      <td>3.8000</td>\n",
              "      <td>2.9500</td>\n",
              "      <td>12.3400</td>\n",
              "      <td>2.7000</td>\n",
              "      <td>29.7100</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>11.1800</td>\n",
              "      <td>1.5500</td>\n",
              "      <td>17.0500</td>\n",
              "      <td>2.0700</td>\n",
              "      <td>12.6400</td>\n",
              "      <td>1.740</td>\n",
              "      <td>1.0800</td>\n",
              "      <td>1.6200</td>\n",
              "      <td>0.8700</td>\n",
              "      <td>2.1300</td>\n",
              "      <td>8.2400</td>\n",
              "      <td>1.9600</td>\n",
              "      <td>4.6100</td>\n",
              "      <td>1.3200</td>\n",
              "      <td>4.4300</td>\n",
              "      <td>3.5100</td>\n",
              "      <td>9.3900</td>\n",
              "      <td>2.5200</td>\n",
              "      <td>12.3800</td>\n",
              "      <td>2.2900</td>\n",
              "      <td>12.1600</td>\n",
              "      <td>2.8200</td>\n",
              "      <td>6.9100</td>\n",
              "      <td>2.9400</td>\n",
              "      <td>2.4700</td>\n",
              "      <td>2.3900</td>\n",
              "      <td>9.1800</td>\n",
              "      <td>2.3300</td>\n",
              "      <td>10.4100</td>\n",
              "      <td>2.7200</td>\n",
              "      <td>6.9500</td>\n",
              "      <td>1.5700</td>\n",
              "      <td>6.3700</td>\n",
              "      <td>2.5600</td>\n",
              "      <td>13.6200</td>\n",
              "      <td>2.5000</td>\n",
              "      <td>5.0100</td>\n",
              "      <td>1.6700</td>\n",
              "      <td>4.4300</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>4.5100</td>\n",
              "      <td>1.7700</td>\n",
              "      <td>2.7800</td>\n",
              "      <td>2.1700</td>\n",
              "      <td>4.3700</td>\n",
              "      <td>1.9200</td>\n",
              "      <td>2.5500</td>\n",
              "      <td>1.6600</td>\n",
              "      <td>11.2600</td>\n",
              "      <td>1.2900</td>\n",
              "      <td>4.5800</td>\n",
              "      <td>2.2200</td>\n",
              "      <td>14.1200</td>\n",
              "      <td>1.7400</td>\n",
              "      <td>9.8200</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>2.8200</td>\n",
              "      <td>3.0200</td>\n",
              "      <td>9.5000</td>\n",
              "      <td>2.5300</td>\n",
              "      <td>33.0000</td>\n",
              "      <td>2.5400</td>\n",
              "      <td>13.6900</td>\n",
              "      <td>1.5600</td>\n",
              "      <td>16.2300</td>\n",
              "      <td>2.0700</td>\n",
              "      <td>10.1100</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>1.0100</td>\n",
              "      <td>1.5000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>72</td>\n",
              "      <td>3</td>\n",
              "      <td>30.4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1494.650</td>\n",
              "      <td>1115.330</td>\n",
              "      <td>478.960</td>\n",
              "      <td>376.840</td>\n",
              "      <td>638.850</td>\n",
              "      <td>476.480</td>\n",
              "      <td>2.4800</td>\n",
              "      <td>1248.03</td>\n",
              "      <td>965.820</td>\n",
              "      <td>282.210</td>\n",
              "      <td>535.920</td>\n",
              "      <td>427.430</td>\n",
              "      <td>3.5300</td>\n",
              "      <td>3.3800</td>\n",
              "      <td>9.0400</td>\n",
              "      <td>10.8300</td>\n",
              "      <td>1.1100</td>\n",
              "      <td>95.9200</td>\n",
              "      <td>99.6300</td>\n",
              "      <td>24.4200</td>\n",
              "      <td>31.95</td>\n",
              "      <td>51.1500</td>\n",
              "      <td>50.8600</td>\n",
              "      <td>71.1700</td>\n",
              "      <td>65.1600</td>\n",
              "      <td>3.6700</td>\n",
              "      <td>2.5500</td>\n",
              "      <td>4.1700</td>\n",
              "      <td>1.7200</td>\n",
              "      <td>3.0800</td>\n",
              "      <td>0.8700</td>\n",
              "      <td>4.8700</td>\n",
              "      <td>2.8800</td>\n",
              "      <td>10.5300</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>11.6100</td>\n",
              "      <td>2.0300</td>\n",
              "      <td>11.8700</td>\n",
              "      <td>2.9900</td>\n",
              "      <td>6.6500</td>\n",
              "      <td>2.840</td>\n",
              "      <td>3.1200</td>\n",
              "      <td>2.1900</td>\n",
              "      <td>8.5500</td>\n",
              "      <td>1.8900</td>\n",
              "      <td>9.9900</td>\n",
              "      <td>2.2100</td>\n",
              "      <td>6.0900</td>\n",
              "      <td>1.2400</td>\n",
              "      <td>5.1400</td>\n",
              "      <td>2.1900</td>\n",
              "      <td>16.1700</td>\n",
              "      <td>2.3200</td>\n",
              "      <td>3.0700</td>\n",
              "      <td>1.0200</td>\n",
              "      <td>4.5900</td>\n",
              "      <td>1.4800</td>\n",
              "      <td>4.4400</td>\n",
              "      <td>1.6800</td>\n",
              "      <td>2.1100</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>5.3800</td>\n",
              "      <td>1.8400</td>\n",
              "      <td>1.5800</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>10.0200</td>\n",
              "      <td>1.2900</td>\n",
              "      <td>3.6800</td>\n",
              "      <td>2.1100</td>\n",
              "      <td>11.0900</td>\n",
              "      <td>1.3100</td>\n",
              "      <td>10.6900</td>\n",
              "      <td>1.6200</td>\n",
              "      <td>4.3100</td>\n",
              "      <td>2.5300</td>\n",
              "      <td>14.2700</td>\n",
              "      <td>2.3600</td>\n",
              "      <td>26.1900</td>\n",
              "      <td>2.3500</td>\n",
              "      <td>10.3200</td>\n",
              "      <td>1.4700</td>\n",
              "      <td>17.0600</td>\n",
              "      <td>1.7600</td>\n",
              "      <td>12.0200</td>\n",
              "      <td>1.820</td>\n",
              "      <td>0.8200</td>\n",
              "      <td>1.0300</td>\n",
              "      <td>2.1100</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>6.8200</td>\n",
              "      <td>1.8700</td>\n",
              "      <td>3.7300</td>\n",
              "      <td>1.1800</td>\n",
              "      <td>4.6800</td>\n",
              "      <td>3.2900</td>\n",
              "      <td>9.0800</td>\n",
              "      <td>2.2700</td>\n",
              "      <td>13.4000</td>\n",
              "      <td>1.9600</td>\n",
              "      <td>11.6800</td>\n",
              "      <td>2.8300</td>\n",
              "      <td>7.2800</td>\n",
              "      <td>2.6600</td>\n",
              "      <td>2.9700</td>\n",
              "      <td>2.2200</td>\n",
              "      <td>12.2200</td>\n",
              "      <td>1.6200</td>\n",
              "      <td>8.4700</td>\n",
              "      <td>2.3200</td>\n",
              "      <td>7.3800</td>\n",
              "      <td>1.2900</td>\n",
              "      <td>5.2300</td>\n",
              "      <td>2.3400</td>\n",
              "      <td>14.6800</td>\n",
              "      <td>2.2400</td>\n",
              "      <td>3.7500</td>\n",
              "      <td>1.1000</td>\n",
              "      <td>4.6600</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>4.7600</td>\n",
              "      <td>1.6400</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>2.0700</td>\n",
              "      <td>5.7500</td>\n",
              "      <td>1.9600</td>\n",
              "      <td>1.9900</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>9.3500</td>\n",
              "      <td>1.1500</td>\n",
              "      <td>3.0800</td>\n",
              "      <td>1.9600</td>\n",
              "      <td>12.2000</td>\n",
              "      <td>1.3200</td>\n",
              "      <td>9.7900</td>\n",
              "      <td>1.7900</td>\n",
              "      <td>2.6200</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>13.1600</td>\n",
              "      <td>2.5100</td>\n",
              "      <td>31.6800</td>\n",
              "      <td>2.4100</td>\n",
              "      <td>9.6400</td>\n",
              "      <td>1.4500</td>\n",
              "      <td>15.1100</td>\n",
              "      <td>1.6400</td>\n",
              "      <td>10.3900</td>\n",
              "      <td>1.8300</td>\n",
              "      <td>0.7400</td>\n",
              "      <td>1.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>82</td>\n",
              "      <td>9</td>\n",
              "      <td>20.8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1494.067</td>\n",
              "      <td>1014.743</td>\n",
              "      <td>444.578</td>\n",
              "      <td>471.128</td>\n",
              "      <td>578.361</td>\n",
              "      <td>436.382</td>\n",
              "      <td>8.1956</td>\n",
              "      <td>1281.46</td>\n",
              "      <td>885.615</td>\n",
              "      <td>395.844</td>\n",
              "      <td>486.349</td>\n",
              "      <td>391.074</td>\n",
              "      <td>2.8706</td>\n",
              "      <td>2.9756</td>\n",
              "      <td>38.7750</td>\n",
              "      <td>38.4131</td>\n",
              "      <td>1.6669</td>\n",
              "      <td>93.0694</td>\n",
              "      <td>91.7550</td>\n",
              "      <td>29.1394</td>\n",
              "      <td>27.87</td>\n",
              "      <td>38.4169</td>\n",
              "      <td>40.5619</td>\n",
              "      <td>57.0300</td>\n",
              "      <td>59.4094</td>\n",
              "      <td>3.9075</td>\n",
              "      <td>2.4561</td>\n",
              "      <td>5.5219</td>\n",
              "      <td>2.1456</td>\n",
              "      <td>3.8925</td>\n",
              "      <td>0.9288</td>\n",
              "      <td>4.8000</td>\n",
              "      <td>2.1765</td>\n",
              "      <td>8.9494</td>\n",
              "      <td>1.7894</td>\n",
              "      <td>9.4200</td>\n",
              "      <td>1.3756</td>\n",
              "      <td>9.4013</td>\n",
              "      <td>2.3596</td>\n",
              "      <td>5.2631</td>\n",
              "      <td>2.631</td>\n",
              "      <td>2.4394</td>\n",
              "      <td>1.4295</td>\n",
              "      <td>10.4813</td>\n",
              "      <td>1.2573</td>\n",
              "      <td>9.3356</td>\n",
              "      <td>2.2364</td>\n",
              "      <td>7.4419</td>\n",
              "      <td>0.9148</td>\n",
              "      <td>3.8363</td>\n",
              "      <td>1.7871</td>\n",
              "      <td>11.5200</td>\n",
              "      <td>2.1492</td>\n",
              "      <td>3.3281</td>\n",
              "      <td>0.9520</td>\n",
              "      <td>3.5325</td>\n",
              "      <td>1.3397</td>\n",
              "      <td>4.8000</td>\n",
              "      <td>1.7196</td>\n",
              "      <td>2.0775</td>\n",
              "      <td>2.1237</td>\n",
              "      <td>3.9863</td>\n",
              "      <td>1.7105</td>\n",
              "      <td>1.5694</td>\n",
              "      <td>0.8144</td>\n",
              "      <td>7.1269</td>\n",
              "      <td>1.1084</td>\n",
              "      <td>3.6975</td>\n",
              "      <td>1.5174</td>\n",
              "      <td>11.9944</td>\n",
              "      <td>1.4951</td>\n",
              "      <td>6.8494</td>\n",
              "      <td>0.9259</td>\n",
              "      <td>4.6744</td>\n",
              "      <td>2.4487</td>\n",
              "      <td>13.4606</td>\n",
              "      <td>2.1809</td>\n",
              "      <td>23.9400</td>\n",
              "      <td>1.9885</td>\n",
              "      <td>8.6888</td>\n",
              "      <td>1.0863</td>\n",
              "      <td>13.7925</td>\n",
              "      <td>1.6908</td>\n",
              "      <td>9.8719</td>\n",
              "      <td>1.728</td>\n",
              "      <td>0.8719</td>\n",
              "      <td>0.8916</td>\n",
              "      <td>2.5406</td>\n",
              "      <td>2.8680</td>\n",
              "      <td>8.2894</td>\n",
              "      <td>1.8736</td>\n",
              "      <td>3.9919</td>\n",
              "      <td>1.0694</td>\n",
              "      <td>3.5663</td>\n",
              "      <td>2.4244</td>\n",
              "      <td>7.3200</td>\n",
              "      <td>2.1698</td>\n",
              "      <td>10.3575</td>\n",
              "      <td>1.4139</td>\n",
              "      <td>11.7469</td>\n",
              "      <td>2.5054</td>\n",
              "      <td>5.8650</td>\n",
              "      <td>2.9249</td>\n",
              "      <td>2.7675</td>\n",
              "      <td>1.6418</td>\n",
              "      <td>9.0488</td>\n",
              "      <td>1.3577</td>\n",
              "      <td>8.4000</td>\n",
              "      <td>1.9227</td>\n",
              "      <td>7.6931</td>\n",
              "      <td>1.1756</td>\n",
              "      <td>3.5831</td>\n",
              "      <td>1.8670</td>\n",
              "      <td>13.0125</td>\n",
              "      <td>2.1627</td>\n",
              "      <td>3.6825</td>\n",
              "      <td>0.9625</td>\n",
              "      <td>4.1813</td>\n",
              "      <td>1.4396</td>\n",
              "      <td>4.0481</td>\n",
              "      <td>1.3973</td>\n",
              "      <td>1.7513</td>\n",
              "      <td>1.6728</td>\n",
              "      <td>3.5325</td>\n",
              "      <td>1.6589</td>\n",
              "      <td>2.0156</td>\n",
              "      <td>0.9432</td>\n",
              "      <td>8.1525</td>\n",
              "      <td>1.1322</td>\n",
              "      <td>3.9956</td>\n",
              "      <td>1.8629</td>\n",
              "      <td>12.0750</td>\n",
              "      <td>1.1521</td>\n",
              "      <td>6.7988</td>\n",
              "      <td>1.3263</td>\n",
              "      <td>3.1856</td>\n",
              "      <td>2.2584</td>\n",
              "      <td>11.1506</td>\n",
              "      <td>1.9614</td>\n",
              "      <td>28.0631</td>\n",
              "      <td>1.9501</td>\n",
              "      <td>9.1125</td>\n",
              "      <td>1.0427</td>\n",
              "      <td>13.3763</td>\n",
              "      <td>1.6270</td>\n",
              "      <td>7.5844</td>\n",
              "      <td>1.4595</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.8527</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa180bbe-f179-40f9-882e-b96cb9af3266')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa180bbe-f179-40f9-882e-b96cb9af3266 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa180bbe-f179-40f9-882e-b96cb9af3266');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TASK-9A: Please apply the following feature engineering steps on the numeric features of the AD dataset.\n",
        "\n",
        "-Divide by NACCBRNV (MRI)\n",
        "\n",
        "-Multiply by NACCAGEB\n"
      ],
      "metadata": {
        "id": "Riz9kAwWdXL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vol_colum = [\"NACCICV\",\"NACCWMVL\",\"CSFVOL\",\"GRAYVOL\",\"WHITEVOL\",\"WMHVOL\",\"CEREALL\",\"CERETISS\",\"CERECSF\",\n",
        "          \"CEREGR\",\"CEREWH\",\"LHIPPO\",\"RHIPPO\",\"LLATVENT\",\"RLATVENT\",\"THIRVENT\",\"LFRCORT\",\"RFRCORT\",\n",
        "          \"LOCCORT\",\"ROCCORT\",\"LPARCORT\",\"RPARCORT\",\n",
        "          \"LTEMPCOR\",\t\"RTEMPCOR\",\t\"LCAC\",\t\"LCACM\",\t\"LCMF\",\t\"LCMFM\",\t\"LCUN\",\t\"LCUNM\",\t\"LENT\",\t\"LENTM\", \"LFUS\",\t\"LFUSM\",\n",
        "          \"LINFPAR\",\t\"LINFPARM\",\t\"LINFTEMP\",\t\"LINFTEMM\",\t\"LINSULA\",\t\"LINSULAM\",\t\"LISTHC\",\t\"LISTHCM\",\t\"LLATOCC\",\t\"LLATOCCM\",\n",
        "          \"LLATORBF\",\t\"LLATORBM\",\t\"LLING\",\t\"LLINGM\",\t\"LMEDORBF\",\t\"LMEDORBM\",\t\"LMIDTEMP\",\t\"LMIDTEMM\",\t\"LPARCEN\",\t\"LPARCENM\",\n",
        "          \"LPARHIP\",\t\"LPARHIPM\",\t\"LPARSOP\",\t\"LPARSOPM\",\t\"LPARORB\"\t,\"LPARORBM\",\t\"LPARTRI\",\t\"LPARTRIM\",\t\"LPERCAL\",\t\"LPERCALM\",\n",
        "          \"LPOSCEN\",\t\"LPOSCENM\",\t\"LPOSCIN\",\t\"LPOSCINM\",\t\"LPRECEN\",\t\"LPRECENM\",\t\"LPRECUN\",\t\"LPRECUNM\",\t\"LROSANC\",\t\"LROSANCM\",\n",
        "          \"LROSMF\",\t\"LROSMFM\",\t\"LSUPFR\",\t\"LSUPFRM\",\t\"LSUPPAR\",\t\"LSUPPARM\",\t\"LSUPTEM\",\t\"LSUPTEMM\",\t\"LSUPMAR\",\t\"LSUPMARM\",\t\"LTRTEM\",\t\"LTRTEMM\"]"
      ],
      "metadata": {
        "id": "PN5w3v8_3iso"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[vol_colum] = data[vol_colum].div(data[\"NACCBRNV\"], axis=0)"
      ],
      "metadata": {
        "id": "8vQPLh9e0LaX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[vol_colum].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "xwXJtkhw5Jpp",
        "outputId": "60bcf132-d96b-437c-d7b0-7e3e56cad765"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    NACCICV  NACCWMVL    CSFVOL   GRAYVOL  WHITEVOL    WMHVOL   CEREALL  \\\n",
              "0  1.250353  0.430592  0.249987  0.569773  0.430227  0.000365  1.062829   \n",
              "1  1.424762  0.431451  0.423173  0.570138  0.429862  0.001589  1.248402   \n",
              "2  1.225458  0.444088  0.224296  0.557074  0.442926  0.001162  1.049901   \n",
              "3  1.340097  0.429433  0.337873  0.572790  0.427210  0.002224  1.118978   \n",
              "4  1.472360  0.438119  0.464283  0.569958  0.430042  0.008077  1.262842   \n",
              "\n",
              "   CERETISS   CERECSF    CEREGR    CEREWH    LHIPPO    RHIPPO  LLATVENT  \\\n",
              "0  0.851212  0.211614  0.462603  0.388244  0.003160  0.003087  0.007484   \n",
              "1  0.880778  0.367624  0.481710  0.397479  0.002889  0.002410  0.023030   \n",
              "2  0.871725  0.178176  0.464681  0.405890  0.002677  0.002745  0.003393   \n",
              "3  0.865950  0.253028  0.480504  0.383232  0.003165  0.003030  0.008105   \n",
              "4  0.872748  0.390093  0.479283  0.385392  0.002829  0.002932  0.038212   \n",
              "\n",
              "   RLATVENT  THIRVENT   LFRCORT   RFRCORT   LOCCORT   ROCCORT  LPARCORT  \\\n",
              "0  0.006088  0.000736  0.080344  0.081307  0.025719  0.027401  0.046414   \n",
              "1  0.050854  0.001824  0.088907  0.084528  0.030767  0.029801  0.041646   \n",
              "2  0.003283  0.000631  0.088696  0.086785  0.026815  0.024786  0.044041   \n",
              "3  0.009710  0.000995  0.086001  0.089328  0.021895  0.028646  0.045861   \n",
              "4  0.037855  0.001643  0.091717  0.090422  0.028716  0.027465  0.037859   \n",
              "\n",
              "   RPARCORT  LTEMPCOR  RTEMPCOR      LCAC     LCACM      LCMF     LCMFM  \\\n",
              "0  0.045462  0.060818  0.055935  0.002210  0.001755  0.003060  0.001555   \n",
              "1  0.041275  0.057480  0.048444  0.002763  0.002690  0.004902  0.001679   \n",
              "2  0.047333  0.054784  0.055095  0.002686  0.001979  0.005153  0.001583   \n",
              "3  0.045601  0.063811  0.058422  0.003291  0.002286  0.003739  0.001542   \n",
              "4  0.039973  0.056201  0.058546  0.003851  0.002420  0.005442  0.002114   \n",
              "\n",
              "       LCUN     LCUNM      LENT     LENTM      LFUS     LFUSM   LINFPAR  \\\n",
              "0  0.003613  0.001090  0.004513  0.002999  0.009597  0.002269  0.011035   \n",
              "1  0.004857  0.001264  0.003548  0.002862  0.008459  0.001869  0.009931   \n",
              "2  0.003502  0.000985  0.004109  0.002972  0.008621  0.002374  0.009859   \n",
              "3  0.002762  0.000780  0.004366  0.002582  0.009441  0.002250  0.010409   \n",
              "4  0.003836  0.000915  0.004730  0.002145  0.008819  0.001763  0.009283   \n",
              "\n",
              "   LINFPARM  LINFTEMP  LINFTEMM   LINSULA  LINSULAM    LISTHC   LISTHCM  \\\n",
              "0  0.002076  0.011189  0.002668  0.005486  0.002501  0.002168  0.001974   \n",
              "1  0.001598  0.009109  0.002284  0.005949  0.003079  0.002889  0.001905   \n",
              "2  0.001802  0.009059  0.002408  0.005515  0.002130  0.002593  0.002054   \n",
              "3  0.001820  0.010643  0.002681  0.005962  0.002546  0.002797  0.001964   \n",
              "4  0.001356  0.009265  0.002325  0.005187  0.002593  0.002404  0.001409   \n",
              "\n",
              "    LLATOCC  LLATOCCM  LLATORBF  LLATORBM     LLING    LLINGM  LMEDORBF  \\\n",
              "0  0.010019  0.001695  0.007164  0.002230  0.005599  0.001211  0.004077   \n",
              "1  0.010490  0.001282  0.007872  0.002176  0.006527  0.001165  0.004207   \n",
              "2  0.010221  0.001734  0.007897  0.002164  0.005456  0.001288  0.004950   \n",
              "3  0.007666  0.001695  0.008957  0.001981  0.005460  0.001112  0.004609   \n",
              "4  0.010329  0.001239  0.009200  0.002204  0.007334  0.000902  0.003781   \n",
              "\n",
              "   LMEDORBM  LMIDTEMP  LMIDTEMM   LPARCEN  LPARCENM   LPARHIP  LPARHIPM  \\\n",
              "0  0.002017  0.012838  0.002446  0.002768  0.001160  0.004030  0.001732   \n",
              "1  0.002167  0.011303  0.001688  0.004830  0.001806  0.003521  0.001589   \n",
              "2  0.002197  0.010894  0.002231  0.003679  0.001204  0.003182  0.001793   \n",
              "3  0.001964  0.014498  0.002080  0.002753  0.000915  0.004115  0.001327   \n",
              "4  0.001761  0.011353  0.002118  0.003280  0.000938  0.003481  0.001320   \n",
              "\n",
              "    LPARSOP  LPARSOPM   LPARORB  LPARORBM   LPARTRI  LPARTRIM   LPERCAL  \\\n",
              "0  0.004603  0.001768  0.001781  0.001840  0.004627  0.001640  0.001892   \n",
              "1  0.005408  0.001887  0.001426  0.001869  0.003810  0.001815  0.003033   \n",
              "2  0.003763  0.001406  0.001987  0.001928  0.003595  0.001726  0.002046   \n",
              "3  0.003981  0.001506  0.001892  0.001632  0.004824  0.001650  0.001417   \n",
              "4  0.004730  0.001695  0.002047  0.002093  0.003928  0.001686  0.001547   \n",
              "\n",
              "   LPERCALM   LPOSCEN  LPOSCENM   LPOSCIN  LPOSCINM   LPRECEN  LPRECENM  \\\n",
              "0  0.001119  0.008402  0.001085  0.003375  0.001687  0.009396  0.001323   \n",
              "1  0.001047  0.007936  0.001417  0.004135  0.001914  0.012043  0.001778   \n",
              "2  0.000977  0.008352  0.001120  0.004555  0.001869  0.013765  0.001389   \n",
              "3  0.000672  0.008984  0.001157  0.003299  0.001892  0.009943  0.001175   \n",
              "4  0.000803  0.007023  0.001092  0.003644  0.001495  0.011820  0.001473   \n",
              "\n",
              "    LPRECUN  LPRECUNM   LROSANC  LROSANCM    LROSMF   LROSMFM    LSUPFR  \\\n",
              "0  0.009032  0.001656  0.003276  0.002534  0.010562  0.001902  0.025077   \n",
              "1  0.008459  0.001607  0.004207  0.002997  0.011935  0.001797  0.023346   \n",
              "2  0.008444  0.001448  0.003199  0.002484  0.010389  0.002273  0.025013   \n",
              "3  0.009585  0.001452  0.003864  0.002268  0.012794  0.002116  0.023482   \n",
              "4  0.006750  0.000912  0.004606  0.002413  0.013265  0.002149  0.023592   \n",
              "\n",
              "    LSUPFRM   LSUPPAR  LSUPPARM   LSUPTEM  LSUPTEMM   LSUPMAR  LSUPMARM  \\\n",
              "0  0.001989  0.010850  0.001338  0.013490  0.001746  0.010939  0.001684   \n",
              "1  0.002257  0.010084  0.001661  0.016395  0.002122  0.008351  0.001869   \n",
              "2  0.002113  0.009413  0.001305  0.014355  0.001743  0.010642  0.001465   \n",
              "3  0.002107  0.009253  0.001318  0.015296  0.001578  0.010777  0.001632   \n",
              "4  0.001960  0.008563  0.001071  0.013592  0.001666  0.009728  0.001703   \n",
              "\n",
              "     LTRTEM   LTRTEMM  \n",
              "0  0.000818  0.001137  \n",
              "1  0.001156  0.002022  \n",
              "2  0.000909  0.001364  \n",
              "3  0.000735  0.000923  \n",
              "4  0.000859  0.000879  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbd64059-18f4-4379-8875-6088d7f20e9e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NACCICV</th>\n",
              "      <th>NACCWMVL</th>\n",
              "      <th>CSFVOL</th>\n",
              "      <th>GRAYVOL</th>\n",
              "      <th>WHITEVOL</th>\n",
              "      <th>WMHVOL</th>\n",
              "      <th>CEREALL</th>\n",
              "      <th>CERETISS</th>\n",
              "      <th>CERECSF</th>\n",
              "      <th>CEREGR</th>\n",
              "      <th>CEREWH</th>\n",
              "      <th>LHIPPO</th>\n",
              "      <th>RHIPPO</th>\n",
              "      <th>LLATVENT</th>\n",
              "      <th>RLATVENT</th>\n",
              "      <th>THIRVENT</th>\n",
              "      <th>LFRCORT</th>\n",
              "      <th>RFRCORT</th>\n",
              "      <th>LOCCORT</th>\n",
              "      <th>ROCCORT</th>\n",
              "      <th>LPARCORT</th>\n",
              "      <th>RPARCORT</th>\n",
              "      <th>LTEMPCOR</th>\n",
              "      <th>RTEMPCOR</th>\n",
              "      <th>LCAC</th>\n",
              "      <th>LCACM</th>\n",
              "      <th>LCMF</th>\n",
              "      <th>LCMFM</th>\n",
              "      <th>LCUN</th>\n",
              "      <th>LCUNM</th>\n",
              "      <th>LENT</th>\n",
              "      <th>LENTM</th>\n",
              "      <th>LFUS</th>\n",
              "      <th>LFUSM</th>\n",
              "      <th>LINFPAR</th>\n",
              "      <th>LINFPARM</th>\n",
              "      <th>LINFTEMP</th>\n",
              "      <th>LINFTEMM</th>\n",
              "      <th>LINSULA</th>\n",
              "      <th>LINSULAM</th>\n",
              "      <th>LISTHC</th>\n",
              "      <th>LISTHCM</th>\n",
              "      <th>LLATOCC</th>\n",
              "      <th>LLATOCCM</th>\n",
              "      <th>LLATORBF</th>\n",
              "      <th>LLATORBM</th>\n",
              "      <th>LLING</th>\n",
              "      <th>LLINGM</th>\n",
              "      <th>LMEDORBF</th>\n",
              "      <th>LMEDORBM</th>\n",
              "      <th>LMIDTEMP</th>\n",
              "      <th>LMIDTEMM</th>\n",
              "      <th>LPARCEN</th>\n",
              "      <th>LPARCENM</th>\n",
              "      <th>LPARHIP</th>\n",
              "      <th>LPARHIPM</th>\n",
              "      <th>LPARSOP</th>\n",
              "      <th>LPARSOPM</th>\n",
              "      <th>LPARORB</th>\n",
              "      <th>LPARORBM</th>\n",
              "      <th>LPARTRI</th>\n",
              "      <th>LPARTRIM</th>\n",
              "      <th>LPERCAL</th>\n",
              "      <th>LPERCALM</th>\n",
              "      <th>LPOSCEN</th>\n",
              "      <th>LPOSCENM</th>\n",
              "      <th>LPOSCIN</th>\n",
              "      <th>LPOSCINM</th>\n",
              "      <th>LPRECEN</th>\n",
              "      <th>LPRECENM</th>\n",
              "      <th>LPRECUN</th>\n",
              "      <th>LPRECUNM</th>\n",
              "      <th>LROSANC</th>\n",
              "      <th>LROSANCM</th>\n",
              "      <th>LROSMF</th>\n",
              "      <th>LROSMFM</th>\n",
              "      <th>LSUPFR</th>\n",
              "      <th>LSUPFRM</th>\n",
              "      <th>LSUPPAR</th>\n",
              "      <th>LSUPPARM</th>\n",
              "      <th>LSUPTEM</th>\n",
              "      <th>LSUPTEMM</th>\n",
              "      <th>LSUPMAR</th>\n",
              "      <th>LSUPMARM</th>\n",
              "      <th>LTRTEM</th>\n",
              "      <th>LTRTEMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.250353</td>\n",
              "      <td>0.430592</td>\n",
              "      <td>0.249987</td>\n",
              "      <td>0.569773</td>\n",
              "      <td>0.430227</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>1.062829</td>\n",
              "      <td>0.851212</td>\n",
              "      <td>0.211614</td>\n",
              "      <td>0.462603</td>\n",
              "      <td>0.388244</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>0.003087</td>\n",
              "      <td>0.007484</td>\n",
              "      <td>0.006088</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.080344</td>\n",
              "      <td>0.081307</td>\n",
              "      <td>0.025719</td>\n",
              "      <td>0.027401</td>\n",
              "      <td>0.046414</td>\n",
              "      <td>0.045462</td>\n",
              "      <td>0.060818</td>\n",
              "      <td>0.055935</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.003060</td>\n",
              "      <td>0.001555</td>\n",
              "      <td>0.003613</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.004513</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.009597</td>\n",
              "      <td>0.002269</td>\n",
              "      <td>0.011035</td>\n",
              "      <td>0.002076</td>\n",
              "      <td>0.011189</td>\n",
              "      <td>0.002668</td>\n",
              "      <td>0.005486</td>\n",
              "      <td>0.002501</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.001974</td>\n",
              "      <td>0.010019</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>0.002230</td>\n",
              "      <td>0.005599</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.002017</td>\n",
              "      <td>0.012838</td>\n",
              "      <td>0.002446</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.004030</td>\n",
              "      <td>0.001732</td>\n",
              "      <td>0.004603</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.001781</td>\n",
              "      <td>0.001840</td>\n",
              "      <td>0.004627</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.008402</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>0.001687</td>\n",
              "      <td>0.009396</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.009032</td>\n",
              "      <td>0.001656</td>\n",
              "      <td>0.003276</td>\n",
              "      <td>0.002534</td>\n",
              "      <td>0.010562</td>\n",
              "      <td>0.001902</td>\n",
              "      <td>0.025077</td>\n",
              "      <td>0.001989</td>\n",
              "      <td>0.010850</td>\n",
              "      <td>0.001338</td>\n",
              "      <td>0.013490</td>\n",
              "      <td>0.001746</td>\n",
              "      <td>0.010939</td>\n",
              "      <td>0.001684</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.001137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.424762</td>\n",
              "      <td>0.431451</td>\n",
              "      <td>0.423173</td>\n",
              "      <td>0.570138</td>\n",
              "      <td>0.429862</td>\n",
              "      <td>0.001589</td>\n",
              "      <td>1.248402</td>\n",
              "      <td>0.880778</td>\n",
              "      <td>0.367624</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>0.397479</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>0.002410</td>\n",
              "      <td>0.023030</td>\n",
              "      <td>0.050854</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.088907</td>\n",
              "      <td>0.084528</td>\n",
              "      <td>0.030767</td>\n",
              "      <td>0.029801</td>\n",
              "      <td>0.041646</td>\n",
              "      <td>0.041275</td>\n",
              "      <td>0.057480</td>\n",
              "      <td>0.048444</td>\n",
              "      <td>0.002763</td>\n",
              "      <td>0.002690</td>\n",
              "      <td>0.004902</td>\n",
              "      <td>0.001679</td>\n",
              "      <td>0.004857</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.003548</td>\n",
              "      <td>0.002862</td>\n",
              "      <td>0.008459</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.009931</td>\n",
              "      <td>0.001598</td>\n",
              "      <td>0.009109</td>\n",
              "      <td>0.002284</td>\n",
              "      <td>0.005949</td>\n",
              "      <td>0.003079</td>\n",
              "      <td>0.002889</td>\n",
              "      <td>0.001905</td>\n",
              "      <td>0.010490</td>\n",
              "      <td>0.001282</td>\n",
              "      <td>0.007872</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>0.006527</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.002167</td>\n",
              "      <td>0.011303</td>\n",
              "      <td>0.001688</td>\n",
              "      <td>0.004830</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>0.001589</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.003810</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.003033</td>\n",
              "      <td>0.001047</td>\n",
              "      <td>0.007936</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>0.004135</td>\n",
              "      <td>0.001914</td>\n",
              "      <td>0.012043</td>\n",
              "      <td>0.001778</td>\n",
              "      <td>0.008459</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.004207</td>\n",
              "      <td>0.002997</td>\n",
              "      <td>0.011935</td>\n",
              "      <td>0.001797</td>\n",
              "      <td>0.023346</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>0.010084</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>0.016395</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.002022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.225458</td>\n",
              "      <td>0.444088</td>\n",
              "      <td>0.224296</td>\n",
              "      <td>0.557074</td>\n",
              "      <td>0.442926</td>\n",
              "      <td>0.001162</td>\n",
              "      <td>1.049901</td>\n",
              "      <td>0.871725</td>\n",
              "      <td>0.178176</td>\n",
              "      <td>0.464681</td>\n",
              "      <td>0.405890</td>\n",
              "      <td>0.002677</td>\n",
              "      <td>0.002745</td>\n",
              "      <td>0.003393</td>\n",
              "      <td>0.003283</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.088696</td>\n",
              "      <td>0.086785</td>\n",
              "      <td>0.026815</td>\n",
              "      <td>0.024786</td>\n",
              "      <td>0.044041</td>\n",
              "      <td>0.047333</td>\n",
              "      <td>0.054784</td>\n",
              "      <td>0.055095</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>0.001979</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>0.001583</td>\n",
              "      <td>0.003502</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>0.004109</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>0.008621</td>\n",
              "      <td>0.002374</td>\n",
              "      <td>0.009859</td>\n",
              "      <td>0.001802</td>\n",
              "      <td>0.009059</td>\n",
              "      <td>0.002408</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.002130</td>\n",
              "      <td>0.002593</td>\n",
              "      <td>0.002054</td>\n",
              "      <td>0.010221</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.007897</td>\n",
              "      <td>0.002164</td>\n",
              "      <td>0.005456</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.010894</td>\n",
              "      <td>0.002231</td>\n",
              "      <td>0.003679</td>\n",
              "      <td>0.001204</td>\n",
              "      <td>0.003182</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.003763</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.001987</td>\n",
              "      <td>0.001928</td>\n",
              "      <td>0.003595</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.008352</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.004555</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.013765</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>0.008444</td>\n",
              "      <td>0.001448</td>\n",
              "      <td>0.003199</td>\n",
              "      <td>0.002484</td>\n",
              "      <td>0.010389</td>\n",
              "      <td>0.002273</td>\n",
              "      <td>0.025013</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.009413</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>0.014355</td>\n",
              "      <td>0.001743</td>\n",
              "      <td>0.010642</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>0.000909</td>\n",
              "      <td>0.001364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.340097</td>\n",
              "      <td>0.429433</td>\n",
              "      <td>0.337873</td>\n",
              "      <td>0.572790</td>\n",
              "      <td>0.427210</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>1.118978</td>\n",
              "      <td>0.865950</td>\n",
              "      <td>0.253028</td>\n",
              "      <td>0.480504</td>\n",
              "      <td>0.383232</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.003030</td>\n",
              "      <td>0.008105</td>\n",
              "      <td>0.009710</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>0.086001</td>\n",
              "      <td>0.089328</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.028646</td>\n",
              "      <td>0.045861</td>\n",
              "      <td>0.045601</td>\n",
              "      <td>0.063811</td>\n",
              "      <td>0.058422</td>\n",
              "      <td>0.003291</td>\n",
              "      <td>0.002286</td>\n",
              "      <td>0.003739</td>\n",
              "      <td>0.001542</td>\n",
              "      <td>0.002762</td>\n",
              "      <td>0.000780</td>\n",
              "      <td>0.004366</td>\n",
              "      <td>0.002582</td>\n",
              "      <td>0.009441</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>0.010409</td>\n",
              "      <td>0.001820</td>\n",
              "      <td>0.010643</td>\n",
              "      <td>0.002681</td>\n",
              "      <td>0.005962</td>\n",
              "      <td>0.002546</td>\n",
              "      <td>0.002797</td>\n",
              "      <td>0.001964</td>\n",
              "      <td>0.007666</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.008957</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.005460</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.004609</td>\n",
              "      <td>0.001964</td>\n",
              "      <td>0.014498</td>\n",
              "      <td>0.002080</td>\n",
              "      <td>0.002753</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.004115</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.003981</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.001632</td>\n",
              "      <td>0.004824</td>\n",
              "      <td>0.001650</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.008984</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.003299</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.009943</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>0.001452</td>\n",
              "      <td>0.003864</td>\n",
              "      <td>0.002268</td>\n",
              "      <td>0.012794</td>\n",
              "      <td>0.002116</td>\n",
              "      <td>0.023482</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.009253</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.015296</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.010777</td>\n",
              "      <td>0.001632</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.472360</td>\n",
              "      <td>0.438119</td>\n",
              "      <td>0.464283</td>\n",
              "      <td>0.569958</td>\n",
              "      <td>0.430042</td>\n",
              "      <td>0.008077</td>\n",
              "      <td>1.262842</td>\n",
              "      <td>0.872748</td>\n",
              "      <td>0.390093</td>\n",
              "      <td>0.479283</td>\n",
              "      <td>0.385392</td>\n",
              "      <td>0.002829</td>\n",
              "      <td>0.002932</td>\n",
              "      <td>0.038212</td>\n",
              "      <td>0.037855</td>\n",
              "      <td>0.001643</td>\n",
              "      <td>0.091717</td>\n",
              "      <td>0.090422</td>\n",
              "      <td>0.028716</td>\n",
              "      <td>0.027465</td>\n",
              "      <td>0.037859</td>\n",
              "      <td>0.039973</td>\n",
              "      <td>0.056201</td>\n",
              "      <td>0.058546</td>\n",
              "      <td>0.003851</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.005442</td>\n",
              "      <td>0.002114</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.008819</td>\n",
              "      <td>0.001763</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>0.001356</td>\n",
              "      <td>0.009265</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.005187</td>\n",
              "      <td>0.002593</td>\n",
              "      <td>0.002404</td>\n",
              "      <td>0.001409</td>\n",
              "      <td>0.010329</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>0.002204</td>\n",
              "      <td>0.007334</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.011353</td>\n",
              "      <td>0.002118</td>\n",
              "      <td>0.003280</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.003481</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>0.001695</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002093</td>\n",
              "      <td>0.003928</td>\n",
              "      <td>0.001686</td>\n",
              "      <td>0.001547</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.007023</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.003644</td>\n",
              "      <td>0.001495</td>\n",
              "      <td>0.011820</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.006750</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.004606</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.013265</td>\n",
              "      <td>0.002149</td>\n",
              "      <td>0.023592</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.008563</td>\n",
              "      <td>0.001071</td>\n",
              "      <td>0.013592</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>0.009728</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.000879</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbd64059-18f4-4379-8875-6088d7f20e9e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dbd64059-18f4-4379-8875-6088d7f20e9e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dbd64059-18f4-4379-8875-6088d7f20e9e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns = \"NACCBRNV\")"
      ],
      "metadata": {
        "id": "_Ud6274S5fop"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[vol_colum] = data[vol_colum].mul(data[\"NACCAGEB\"], axis=0)"
      ],
      "metadata": {
        "id": "vYjlfBmv5m4P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[vol_colum].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "BynM4WI76PsJ",
        "outputId": "c2ef91ed-1c90-4dda-e8f8-f202640ead5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      NACCICV   NACCWMVL     CSFVOL    GRAYVOL   WHITEVOL    WMHVOL  \\\n",
              "0   70.019756  24.113158  13.999287  31.907311  24.092689  0.020460   \n",
              "1   92.609508  28.044336  27.506229  37.058943  27.941057  0.103279   \n",
              "2   71.076564  25.757106  13.009177  32.310282  25.689718  0.067387   \n",
              "3   96.486959  30.919208  24.326863  41.240888  30.759112  0.160096   \n",
              "4  120.733520  35.925743  38.071212  46.736565  35.263435  0.662275   \n",
              "\n",
              "      CEREALL   CERETISS    CERECSF     CEREGR     CEREWH    LHIPPO    RHIPPO  \\\n",
              "0   59.518414  47.667882  11.850382  25.905772  21.741640  0.176984  0.172855   \n",
              "1   81.146134  57.250560  23.895575  31.311119  25.836162  0.187780  0.156679   \n",
              "2   60.894238  50.560046  10.334192  26.951522  23.541625  0.155284  0.159190   \n",
              "3   80.566433  62.348399  18.218034  34.596254  27.592695  0.227879  0.218196   \n",
              "4  103.553038  71.565342  31.987615  39.301200  31.602157  0.231969  0.240454   \n",
              "\n",
              "   LLATVENT  RLATVENT  THIRVENT   LFRCORT   RFRCORT   LOCCORT   ROCCORT  \\\n",
              "0  0.419091  0.340921  0.041194  4.499242  4.553199  1.440252  1.534467   \n",
              "1  1.496958  3.305512  0.118536  5.778925  5.494321  1.999856  1.937067   \n",
              "2  0.196791  0.190443  0.036624  5.144389  5.033542  1.555281  1.437597   \n",
              "3  0.583576  0.699129  0.071656  6.192105  6.431603  1.576430  2.062529   \n",
              "4  3.133355  3.104110  0.134700  7.520811  7.414597  2.354715  2.252137   \n",
              "\n",
              "   LPARCORT  RPARCORT  LTEMPCOR  RTEMPCOR      LCAC     LCACM      LCMF  \\\n",
              "0  2.599175  2.545874  3.405822  3.132370  0.123773  0.098274  0.171354   \n",
              "1  2.706964  2.682905  3.736232  3.148834  0.179564  0.174870  0.318639   \n",
              "2  2.554371  2.745302  3.177460  3.195528  0.155772  0.114754  0.298848   \n",
              "3  3.301982  3.283261  4.594371  4.206396  0.236916  0.164615  0.269194   \n",
              "4  3.104417  3.277752  4.608517  4.800793  0.315760  0.198474  0.446217   \n",
              "\n",
              "      LCMFM      LCUN     LCUNM      LENT     LENTM      LFUS     LFUSM  \\\n",
              "0  0.087058  0.202318  0.061043  0.252712  0.167946  0.537419  0.127082   \n",
              "1  0.109147  0.315705  0.082154  0.230617  0.186019  0.549843  0.121470   \n",
              "2  0.091803  0.203139  0.057133  0.238297  0.172375  0.500034  0.137705   \n",
              "3  0.111034  0.198829  0.056163  0.314382  0.185918  0.679763  0.162033   \n",
              "4  0.173383  0.314548  0.075055  0.387881  0.175880  0.723189  0.144599   \n",
              "\n",
              "    LINFPAR  LINFPARM  LINFTEMP  LINFTEMM   LINSULA  LINSULAM    LISTHC  \\\n",
              "0  0.617936  0.116231  0.626569  0.149383  0.307234  0.140084  0.121431   \n",
              "1  0.645493  0.103866  0.592093  0.148463  0.386709  0.200103  0.187780   \n",
              "2  0.571816  0.104499  0.525426  0.139658  0.319846  0.123543  0.150401   \n",
              "3  0.749482  0.131046  0.766266  0.193019  0.429290  0.183336  0.201411   \n",
              "4  0.761217  0.111160  0.759706  0.190676  0.425304  0.212608  0.197125   \n",
              "\n",
              "    LISTHCM   LLATOCC  LLATOCCM  LLATORBF  LLATORBM     LLING    LLINGM  \\\n",
              "0  0.110536  0.561067  0.094896  0.401164  0.124874  0.313520  0.067830   \n",
              "1  0.123817  0.681876  0.083327  0.511700  0.141422  0.424265  0.075699   \n",
              "2  0.119149  0.592813  0.100593  0.458039  0.125497  0.316428  0.074712   \n",
              "3  0.141375  0.551944  0.122009  0.644903  0.142666  0.393139  0.080048   \n",
              "4  0.115516  0.846980  0.101601  0.754397  0.180720  0.601370  0.073924   \n",
              "\n",
              "   LMEDORBF  LMEDORBM  LMIDTEMP  LMIDTEMM   LPARCEN  LPARCENM   LPARHIP  \\\n",
              "0  0.228313  0.112928  0.718907  0.136966  0.155023  0.064957  0.225686   \n",
              "1  0.273454  0.140835  0.734689  0.109734  0.313944  0.117362  0.228857   \n",
              "2  0.287129  0.127450  0.631878  0.129403  0.213393  0.069829  0.184583   \n",
              "3  0.331812  0.141375  1.043852  0.149767  0.198183  0.065846  0.296307   \n",
              "4  0.310006  0.144413  0.930916  0.173674  0.268939  0.076930  0.285457   \n",
              "\n",
              "   LPARHIPM   LPARSOP  LPARSOPM   LPARORB  LPARORBM   LPARTRI  LPARTRIM  \\\n",
              "0  0.096983  0.257776  0.098995  0.099750  0.103044  0.259093  0.091838   \n",
              "1  0.103279  0.351500  0.122644  0.092716  0.121470  0.247635  0.117949   \n",
              "2  0.104011  0.218276  0.081548  0.115242  0.111824  0.208510  0.100104   \n",
              "3  0.095541  0.286624  0.108452  0.136211  0.117490  0.347305  0.118781   \n",
              "4  0.108259  0.387881  0.138959  0.167880  0.171613  0.322127  0.138223   \n",
              "\n",
              "    LPERCAL  LPERCALM   LPOSCEN  LPOSCENM   LPOSCIN  LPOSCINM   LPRECEN  \\\n",
              "0  0.105946  0.062655  0.470515  0.060743  0.188996  0.094465  0.526158   \n",
              "1  0.197169  0.068070  0.515808  0.092129  0.268760  0.124404  0.782807   \n",
              "2  0.118660  0.056644  0.484408  0.064946  0.264178  0.108406  0.798394   \n",
              "3  0.101997  0.048416  0.646840  0.083276  0.237562  0.136211  0.715914   \n",
              "4  0.126821  0.065811  0.575915  0.089568  0.298790  0.122619  0.969251   \n",
              "\n",
              "   LPRECENM   LPRECUN  LPRECUNM   LROSANC  LROSANCM    LROSMF   LROSMFM  \\\n",
              "0  0.074061  0.505799  0.092754  0.183455  0.141916  0.591471  0.106522   \n",
              "1  0.115602  0.549843  0.104453  0.273454  0.194822  0.775766  0.116776   \n",
              "2  0.080572  0.489779  0.083990  0.185559  0.144053  0.602580  0.131845   \n",
              "3  0.084567  0.690092  0.104579  0.278232  0.163324  0.921198  0.152350   \n",
              "4  0.120817  0.553491  0.074821  0.377732  0.197876  1.087733  0.176236   \n",
              "\n",
              "     LSUPFR   LSUPFRM   LSUPPAR  LSUPPARM   LSUPTEM  LSUPTEMM   LSUPMAR  \\\n",
              "0  1.404313  0.111382  0.607611  0.074907  0.755412  0.097799  0.612586   \n",
              "1  1.517496  0.146703  0.655469  0.107973  1.065651  0.137901  0.542801   \n",
              "2  1.450781  0.122567  0.545935  0.075689  0.832576  0.101081  0.617229   \n",
              "3  1.690692  0.151704  0.666206  0.094896  1.101306  0.113617  0.775950   \n",
              "4  1.934559  0.160688  0.702130  0.087782  1.114553  0.136631  0.797735   \n",
              "\n",
              "   LSUPMARM    LTRTEM   LTRTEMM  \n",
              "0  0.094290  0.045794  0.063681  \n",
              "1  0.121470  0.075112  0.131446  \n",
              "2  0.084967  0.052738  0.079107  \n",
              "3  0.117490  0.052935  0.066492  \n",
              "4  0.139637  0.070457  0.072049  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ffe8b59-1b60-4c56-b701-b2ab88ce17c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NACCICV</th>\n",
              "      <th>NACCWMVL</th>\n",
              "      <th>CSFVOL</th>\n",
              "      <th>GRAYVOL</th>\n",
              "      <th>WHITEVOL</th>\n",
              "      <th>WMHVOL</th>\n",
              "      <th>CEREALL</th>\n",
              "      <th>CERETISS</th>\n",
              "      <th>CERECSF</th>\n",
              "      <th>CEREGR</th>\n",
              "      <th>CEREWH</th>\n",
              "      <th>LHIPPO</th>\n",
              "      <th>RHIPPO</th>\n",
              "      <th>LLATVENT</th>\n",
              "      <th>RLATVENT</th>\n",
              "      <th>THIRVENT</th>\n",
              "      <th>LFRCORT</th>\n",
              "      <th>RFRCORT</th>\n",
              "      <th>LOCCORT</th>\n",
              "      <th>ROCCORT</th>\n",
              "      <th>LPARCORT</th>\n",
              "      <th>RPARCORT</th>\n",
              "      <th>LTEMPCOR</th>\n",
              "      <th>RTEMPCOR</th>\n",
              "      <th>LCAC</th>\n",
              "      <th>LCACM</th>\n",
              "      <th>LCMF</th>\n",
              "      <th>LCMFM</th>\n",
              "      <th>LCUN</th>\n",
              "      <th>LCUNM</th>\n",
              "      <th>LENT</th>\n",
              "      <th>LENTM</th>\n",
              "      <th>LFUS</th>\n",
              "      <th>LFUSM</th>\n",
              "      <th>LINFPAR</th>\n",
              "      <th>LINFPARM</th>\n",
              "      <th>LINFTEMP</th>\n",
              "      <th>LINFTEMM</th>\n",
              "      <th>LINSULA</th>\n",
              "      <th>LINSULAM</th>\n",
              "      <th>LISTHC</th>\n",
              "      <th>LISTHCM</th>\n",
              "      <th>LLATOCC</th>\n",
              "      <th>LLATOCCM</th>\n",
              "      <th>LLATORBF</th>\n",
              "      <th>LLATORBM</th>\n",
              "      <th>LLING</th>\n",
              "      <th>LLINGM</th>\n",
              "      <th>LMEDORBF</th>\n",
              "      <th>LMEDORBM</th>\n",
              "      <th>LMIDTEMP</th>\n",
              "      <th>LMIDTEMM</th>\n",
              "      <th>LPARCEN</th>\n",
              "      <th>LPARCENM</th>\n",
              "      <th>LPARHIP</th>\n",
              "      <th>LPARHIPM</th>\n",
              "      <th>LPARSOP</th>\n",
              "      <th>LPARSOPM</th>\n",
              "      <th>LPARORB</th>\n",
              "      <th>LPARORBM</th>\n",
              "      <th>LPARTRI</th>\n",
              "      <th>LPARTRIM</th>\n",
              "      <th>LPERCAL</th>\n",
              "      <th>LPERCALM</th>\n",
              "      <th>LPOSCEN</th>\n",
              "      <th>LPOSCENM</th>\n",
              "      <th>LPOSCIN</th>\n",
              "      <th>LPOSCINM</th>\n",
              "      <th>LPRECEN</th>\n",
              "      <th>LPRECENM</th>\n",
              "      <th>LPRECUN</th>\n",
              "      <th>LPRECUNM</th>\n",
              "      <th>LROSANC</th>\n",
              "      <th>LROSANCM</th>\n",
              "      <th>LROSMF</th>\n",
              "      <th>LROSMFM</th>\n",
              "      <th>LSUPFR</th>\n",
              "      <th>LSUPFRM</th>\n",
              "      <th>LSUPPAR</th>\n",
              "      <th>LSUPPARM</th>\n",
              "      <th>LSUPTEM</th>\n",
              "      <th>LSUPTEMM</th>\n",
              "      <th>LSUPMAR</th>\n",
              "      <th>LSUPMARM</th>\n",
              "      <th>LTRTEM</th>\n",
              "      <th>LTRTEMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.019756</td>\n",
              "      <td>24.113158</td>\n",
              "      <td>13.999287</td>\n",
              "      <td>31.907311</td>\n",
              "      <td>24.092689</td>\n",
              "      <td>0.020460</td>\n",
              "      <td>59.518414</td>\n",
              "      <td>47.667882</td>\n",
              "      <td>11.850382</td>\n",
              "      <td>25.905772</td>\n",
              "      <td>21.741640</td>\n",
              "      <td>0.176984</td>\n",
              "      <td>0.172855</td>\n",
              "      <td>0.419091</td>\n",
              "      <td>0.340921</td>\n",
              "      <td>0.041194</td>\n",
              "      <td>4.499242</td>\n",
              "      <td>4.553199</td>\n",
              "      <td>1.440252</td>\n",
              "      <td>1.534467</td>\n",
              "      <td>2.599175</td>\n",
              "      <td>2.545874</td>\n",
              "      <td>3.405822</td>\n",
              "      <td>3.132370</td>\n",
              "      <td>0.123773</td>\n",
              "      <td>0.098274</td>\n",
              "      <td>0.171354</td>\n",
              "      <td>0.087058</td>\n",
              "      <td>0.202318</td>\n",
              "      <td>0.061043</td>\n",
              "      <td>0.252712</td>\n",
              "      <td>0.167946</td>\n",
              "      <td>0.537419</td>\n",
              "      <td>0.127082</td>\n",
              "      <td>0.617936</td>\n",
              "      <td>0.116231</td>\n",
              "      <td>0.626569</td>\n",
              "      <td>0.149383</td>\n",
              "      <td>0.307234</td>\n",
              "      <td>0.140084</td>\n",
              "      <td>0.121431</td>\n",
              "      <td>0.110536</td>\n",
              "      <td>0.561067</td>\n",
              "      <td>0.094896</td>\n",
              "      <td>0.401164</td>\n",
              "      <td>0.124874</td>\n",
              "      <td>0.313520</td>\n",
              "      <td>0.067830</td>\n",
              "      <td>0.228313</td>\n",
              "      <td>0.112928</td>\n",
              "      <td>0.718907</td>\n",
              "      <td>0.136966</td>\n",
              "      <td>0.155023</td>\n",
              "      <td>0.064957</td>\n",
              "      <td>0.225686</td>\n",
              "      <td>0.096983</td>\n",
              "      <td>0.257776</td>\n",
              "      <td>0.098995</td>\n",
              "      <td>0.099750</td>\n",
              "      <td>0.103044</td>\n",
              "      <td>0.259093</td>\n",
              "      <td>0.091838</td>\n",
              "      <td>0.105946</td>\n",
              "      <td>0.062655</td>\n",
              "      <td>0.470515</td>\n",
              "      <td>0.060743</td>\n",
              "      <td>0.188996</td>\n",
              "      <td>0.094465</td>\n",
              "      <td>0.526158</td>\n",
              "      <td>0.074061</td>\n",
              "      <td>0.505799</td>\n",
              "      <td>0.092754</td>\n",
              "      <td>0.183455</td>\n",
              "      <td>0.141916</td>\n",
              "      <td>0.591471</td>\n",
              "      <td>0.106522</td>\n",
              "      <td>1.404313</td>\n",
              "      <td>0.111382</td>\n",
              "      <td>0.607611</td>\n",
              "      <td>0.074907</td>\n",
              "      <td>0.755412</td>\n",
              "      <td>0.097799</td>\n",
              "      <td>0.612586</td>\n",
              "      <td>0.094290</td>\n",
              "      <td>0.045794</td>\n",
              "      <td>0.063681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92.609508</td>\n",
              "      <td>28.044336</td>\n",
              "      <td>27.506229</td>\n",
              "      <td>37.058943</td>\n",
              "      <td>27.941057</td>\n",
              "      <td>0.103279</td>\n",
              "      <td>81.146134</td>\n",
              "      <td>57.250560</td>\n",
              "      <td>23.895575</td>\n",
              "      <td>31.311119</td>\n",
              "      <td>25.836162</td>\n",
              "      <td>0.187780</td>\n",
              "      <td>0.156679</td>\n",
              "      <td>1.496958</td>\n",
              "      <td>3.305512</td>\n",
              "      <td>0.118536</td>\n",
              "      <td>5.778925</td>\n",
              "      <td>5.494321</td>\n",
              "      <td>1.999856</td>\n",
              "      <td>1.937067</td>\n",
              "      <td>2.706964</td>\n",
              "      <td>2.682905</td>\n",
              "      <td>3.736232</td>\n",
              "      <td>3.148834</td>\n",
              "      <td>0.179564</td>\n",
              "      <td>0.174870</td>\n",
              "      <td>0.318639</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.315705</td>\n",
              "      <td>0.082154</td>\n",
              "      <td>0.230617</td>\n",
              "      <td>0.186019</td>\n",
              "      <td>0.549843</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.645493</td>\n",
              "      <td>0.103866</td>\n",
              "      <td>0.592093</td>\n",
              "      <td>0.148463</td>\n",
              "      <td>0.386709</td>\n",
              "      <td>0.200103</td>\n",
              "      <td>0.187780</td>\n",
              "      <td>0.123817</td>\n",
              "      <td>0.681876</td>\n",
              "      <td>0.083327</td>\n",
              "      <td>0.511700</td>\n",
              "      <td>0.141422</td>\n",
              "      <td>0.424265</td>\n",
              "      <td>0.075699</td>\n",
              "      <td>0.273454</td>\n",
              "      <td>0.140835</td>\n",
              "      <td>0.734689</td>\n",
              "      <td>0.109734</td>\n",
              "      <td>0.313944</td>\n",
              "      <td>0.117362</td>\n",
              "      <td>0.228857</td>\n",
              "      <td>0.103279</td>\n",
              "      <td>0.351500</td>\n",
              "      <td>0.122644</td>\n",
              "      <td>0.092716</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.247635</td>\n",
              "      <td>0.117949</td>\n",
              "      <td>0.197169</td>\n",
              "      <td>0.068070</td>\n",
              "      <td>0.515808</td>\n",
              "      <td>0.092129</td>\n",
              "      <td>0.268760</td>\n",
              "      <td>0.124404</td>\n",
              "      <td>0.782807</td>\n",
              "      <td>0.115602</td>\n",
              "      <td>0.549843</td>\n",
              "      <td>0.104453</td>\n",
              "      <td>0.273454</td>\n",
              "      <td>0.194822</td>\n",
              "      <td>0.775766</td>\n",
              "      <td>0.116776</td>\n",
              "      <td>1.517496</td>\n",
              "      <td>0.146703</td>\n",
              "      <td>0.655469</td>\n",
              "      <td>0.107973</td>\n",
              "      <td>1.065651</td>\n",
              "      <td>0.137901</td>\n",
              "      <td>0.542801</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.075112</td>\n",
              "      <td>0.131446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>71.076564</td>\n",
              "      <td>25.757106</td>\n",
              "      <td>13.009177</td>\n",
              "      <td>32.310282</td>\n",
              "      <td>25.689718</td>\n",
              "      <td>0.067387</td>\n",
              "      <td>60.894238</td>\n",
              "      <td>50.560046</td>\n",
              "      <td>10.334192</td>\n",
              "      <td>26.951522</td>\n",
              "      <td>23.541625</td>\n",
              "      <td>0.155284</td>\n",
              "      <td>0.159190</td>\n",
              "      <td>0.196791</td>\n",
              "      <td>0.190443</td>\n",
              "      <td>0.036624</td>\n",
              "      <td>5.144389</td>\n",
              "      <td>5.033542</td>\n",
              "      <td>1.555281</td>\n",
              "      <td>1.437597</td>\n",
              "      <td>2.554371</td>\n",
              "      <td>2.745302</td>\n",
              "      <td>3.177460</td>\n",
              "      <td>3.195528</td>\n",
              "      <td>0.155772</td>\n",
              "      <td>0.114754</td>\n",
              "      <td>0.298848</td>\n",
              "      <td>0.091803</td>\n",
              "      <td>0.203139</td>\n",
              "      <td>0.057133</td>\n",
              "      <td>0.238297</td>\n",
              "      <td>0.172375</td>\n",
              "      <td>0.500034</td>\n",
              "      <td>0.137705</td>\n",
              "      <td>0.571816</td>\n",
              "      <td>0.104499</td>\n",
              "      <td>0.525426</td>\n",
              "      <td>0.139658</td>\n",
              "      <td>0.319846</td>\n",
              "      <td>0.123543</td>\n",
              "      <td>0.150401</td>\n",
              "      <td>0.119149</td>\n",
              "      <td>0.592813</td>\n",
              "      <td>0.100593</td>\n",
              "      <td>0.458039</td>\n",
              "      <td>0.125497</td>\n",
              "      <td>0.316428</td>\n",
              "      <td>0.074712</td>\n",
              "      <td>0.287129</td>\n",
              "      <td>0.127450</td>\n",
              "      <td>0.631878</td>\n",
              "      <td>0.129403</td>\n",
              "      <td>0.213393</td>\n",
              "      <td>0.069829</td>\n",
              "      <td>0.184583</td>\n",
              "      <td>0.104011</td>\n",
              "      <td>0.218276</td>\n",
              "      <td>0.081548</td>\n",
              "      <td>0.115242</td>\n",
              "      <td>0.111824</td>\n",
              "      <td>0.208510</td>\n",
              "      <td>0.100104</td>\n",
              "      <td>0.118660</td>\n",
              "      <td>0.056644</td>\n",
              "      <td>0.484408</td>\n",
              "      <td>0.064946</td>\n",
              "      <td>0.264178</td>\n",
              "      <td>0.108406</td>\n",
              "      <td>0.798394</td>\n",
              "      <td>0.080572</td>\n",
              "      <td>0.489779</td>\n",
              "      <td>0.083990</td>\n",
              "      <td>0.185559</td>\n",
              "      <td>0.144053</td>\n",
              "      <td>0.602580</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>1.450781</td>\n",
              "      <td>0.122567</td>\n",
              "      <td>0.545935</td>\n",
              "      <td>0.075689</td>\n",
              "      <td>0.832576</td>\n",
              "      <td>0.101081</td>\n",
              "      <td>0.617229</td>\n",
              "      <td>0.084967</td>\n",
              "      <td>0.052738</td>\n",
              "      <td>0.079107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>96.486959</td>\n",
              "      <td>30.919208</td>\n",
              "      <td>24.326863</td>\n",
              "      <td>41.240888</td>\n",
              "      <td>30.759112</td>\n",
              "      <td>0.160096</td>\n",
              "      <td>80.566433</td>\n",
              "      <td>62.348399</td>\n",
              "      <td>18.218034</td>\n",
              "      <td>34.596254</td>\n",
              "      <td>27.592695</td>\n",
              "      <td>0.227879</td>\n",
              "      <td>0.218196</td>\n",
              "      <td>0.583576</td>\n",
              "      <td>0.699129</td>\n",
              "      <td>0.071656</td>\n",
              "      <td>6.192105</td>\n",
              "      <td>6.431603</td>\n",
              "      <td>1.576430</td>\n",
              "      <td>2.062529</td>\n",
              "      <td>3.301982</td>\n",
              "      <td>3.283261</td>\n",
              "      <td>4.594371</td>\n",
              "      <td>4.206396</td>\n",
              "      <td>0.236916</td>\n",
              "      <td>0.164615</td>\n",
              "      <td>0.269194</td>\n",
              "      <td>0.111034</td>\n",
              "      <td>0.198829</td>\n",
              "      <td>0.056163</td>\n",
              "      <td>0.314382</td>\n",
              "      <td>0.185918</td>\n",
              "      <td>0.679763</td>\n",
              "      <td>0.162033</td>\n",
              "      <td>0.749482</td>\n",
              "      <td>0.131046</td>\n",
              "      <td>0.766266</td>\n",
              "      <td>0.193019</td>\n",
              "      <td>0.429290</td>\n",
              "      <td>0.183336</td>\n",
              "      <td>0.201411</td>\n",
              "      <td>0.141375</td>\n",
              "      <td>0.551944</td>\n",
              "      <td>0.122009</td>\n",
              "      <td>0.644903</td>\n",
              "      <td>0.142666</td>\n",
              "      <td>0.393139</td>\n",
              "      <td>0.080048</td>\n",
              "      <td>0.331812</td>\n",
              "      <td>0.141375</td>\n",
              "      <td>1.043852</td>\n",
              "      <td>0.149767</td>\n",
              "      <td>0.198183</td>\n",
              "      <td>0.065846</td>\n",
              "      <td>0.296307</td>\n",
              "      <td>0.095541</td>\n",
              "      <td>0.286624</td>\n",
              "      <td>0.108452</td>\n",
              "      <td>0.136211</td>\n",
              "      <td>0.117490</td>\n",
              "      <td>0.347305</td>\n",
              "      <td>0.118781</td>\n",
              "      <td>0.101997</td>\n",
              "      <td>0.048416</td>\n",
              "      <td>0.646840</td>\n",
              "      <td>0.083276</td>\n",
              "      <td>0.237562</td>\n",
              "      <td>0.136211</td>\n",
              "      <td>0.715914</td>\n",
              "      <td>0.084567</td>\n",
              "      <td>0.690092</td>\n",
              "      <td>0.104579</td>\n",
              "      <td>0.278232</td>\n",
              "      <td>0.163324</td>\n",
              "      <td>0.921198</td>\n",
              "      <td>0.152350</td>\n",
              "      <td>1.690692</td>\n",
              "      <td>0.151704</td>\n",
              "      <td>0.666206</td>\n",
              "      <td>0.094896</td>\n",
              "      <td>1.101306</td>\n",
              "      <td>0.113617</td>\n",
              "      <td>0.775950</td>\n",
              "      <td>0.117490</td>\n",
              "      <td>0.052935</td>\n",
              "      <td>0.066492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>120.733520</td>\n",
              "      <td>35.925743</td>\n",
              "      <td>38.071212</td>\n",
              "      <td>46.736565</td>\n",
              "      <td>35.263435</td>\n",
              "      <td>0.662275</td>\n",
              "      <td>103.553038</td>\n",
              "      <td>71.565342</td>\n",
              "      <td>31.987615</td>\n",
              "      <td>39.301200</td>\n",
              "      <td>31.602157</td>\n",
              "      <td>0.231969</td>\n",
              "      <td>0.240454</td>\n",
              "      <td>3.133355</td>\n",
              "      <td>3.104110</td>\n",
              "      <td>0.134700</td>\n",
              "      <td>7.520811</td>\n",
              "      <td>7.414597</td>\n",
              "      <td>2.354715</td>\n",
              "      <td>2.252137</td>\n",
              "      <td>3.104417</td>\n",
              "      <td>3.277752</td>\n",
              "      <td>4.608517</td>\n",
              "      <td>4.800793</td>\n",
              "      <td>0.315760</td>\n",
              "      <td>0.198474</td>\n",
              "      <td>0.446217</td>\n",
              "      <td>0.173383</td>\n",
              "      <td>0.314548</td>\n",
              "      <td>0.075055</td>\n",
              "      <td>0.387881</td>\n",
              "      <td>0.175880</td>\n",
              "      <td>0.723189</td>\n",
              "      <td>0.144599</td>\n",
              "      <td>0.761217</td>\n",
              "      <td>0.111160</td>\n",
              "      <td>0.759706</td>\n",
              "      <td>0.190676</td>\n",
              "      <td>0.425304</td>\n",
              "      <td>0.212608</td>\n",
              "      <td>0.197125</td>\n",
              "      <td>0.115516</td>\n",
              "      <td>0.846980</td>\n",
              "      <td>0.101601</td>\n",
              "      <td>0.754397</td>\n",
              "      <td>0.180720</td>\n",
              "      <td>0.601370</td>\n",
              "      <td>0.073924</td>\n",
              "      <td>0.310006</td>\n",
              "      <td>0.144413</td>\n",
              "      <td>0.930916</td>\n",
              "      <td>0.173674</td>\n",
              "      <td>0.268939</td>\n",
              "      <td>0.076930</td>\n",
              "      <td>0.285457</td>\n",
              "      <td>0.108259</td>\n",
              "      <td>0.387881</td>\n",
              "      <td>0.138959</td>\n",
              "      <td>0.167880</td>\n",
              "      <td>0.171613</td>\n",
              "      <td>0.322127</td>\n",
              "      <td>0.138223</td>\n",
              "      <td>0.126821</td>\n",
              "      <td>0.065811</td>\n",
              "      <td>0.575915</td>\n",
              "      <td>0.089568</td>\n",
              "      <td>0.298790</td>\n",
              "      <td>0.122619</td>\n",
              "      <td>0.969251</td>\n",
              "      <td>0.120817</td>\n",
              "      <td>0.553491</td>\n",
              "      <td>0.074821</td>\n",
              "      <td>0.377732</td>\n",
              "      <td>0.197876</td>\n",
              "      <td>1.087733</td>\n",
              "      <td>0.176236</td>\n",
              "      <td>1.934559</td>\n",
              "      <td>0.160688</td>\n",
              "      <td>0.702130</td>\n",
              "      <td>0.087782</td>\n",
              "      <td>1.114553</td>\n",
              "      <td>0.136631</td>\n",
              "      <td>0.797735</td>\n",
              "      <td>0.139637</td>\n",
              "      <td>0.070457</td>\n",
              "      <td>0.072049</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ffe8b59-1b60-4c56-b701-b2ab88ce17c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ffe8b59-1b60-4c56-b701-b2ab88ce17c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ffe8b59-1b60-4c56-b701-b2ab88ce17c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns = \"NACCAGEB\")"
      ],
      "metadata": {
        "id": "QL3jwwBO6ScD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"CDRGLOB\"].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSmC3hdg7kbC",
        "outputId": "f81afe4c-dd1e-4a0a-8f4e-abbbb8ef59e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK-9B: Please apply Gaussian Naive Bayes algorithm on the numeric values (after the transformation) to predict the CDRGLOB values (class-based). Train-test split, train by grid search (k=5) and optimize for the parameter values."
      ],
      "metadata": {
        "id": "dtgZksruddgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = data[\"CDRGLOB\"]\n",
        "X = data[vol_colum]"
      ],
      "metadata": {
        "id": "4--KoaB_8v5q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "K_RLwtF26Vy_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "Hom_LMLF9Uh6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7dvcgRi-laz",
        "outputId": "158b7b5c-0bd5-41f9-980f-6db5aa1b01b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5350553505535055"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_smoothies = list()\n",
        "a = 1\n",
        "for i in range(20):\n",
        "  my_smoothies.append(a)\n",
        "  a /= 10\n",
        "\n",
        "my_smoothies = sorted(my_smoothies,reverse=False)\n",
        "my_smoothies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xht9TIjY97Up",
        "outputId": "85f87b64-d570-4348-b9ad-3c732599e5f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0000000000000001e-19,\n",
              " 1e-18,\n",
              " 1e-17,\n",
              " 1.0000000000000001e-16,\n",
              " 1e-15,\n",
              " 1.0000000000000002e-14,\n",
              " 1.0000000000000002e-13,\n",
              " 1.0000000000000002e-12,\n",
              " 1.0000000000000003e-11,\n",
              " 1.0000000000000003e-10,\n",
              " 1.0000000000000003e-09,\n",
              " 1.0000000000000002e-08,\n",
              " 1.0000000000000002e-07,\n",
              " 1.0000000000000002e-06,\n",
              " 1e-05,\n",
              " 0.0001,\n",
              " 0.001,\n",
              " 0.01,\n",
              " 0.1,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'var_smoothing': my_smoothies}\n",
        "\n",
        "grid = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "id": "0EnRmFew9zB_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "KcKildIm-zYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gihhpNUc-24P",
        "outputId": "76ccfc27-d58b-412d-89ee-8d52b9684fe0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'var_smoothing': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = grid.best_estimator_\n",
        "my_predictions = gnb.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "74S3IBO7-_yH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = [\"1\",\"2\",\"3\",\"4\"]\n",
        "print(classification_report(y_test, my_predictions, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3omIzmlV_DAw",
        "outputId": "444cb877-3d5a-4a03-a6ea-f6a5bd920ada"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.66      0.69      0.67       128\n",
            "           2       0.49      0.66      0.56       102\n",
            "           3       0.00      0.00      0.00        32\n",
            "           4       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.57       271\n",
            "   macro avg       0.29      0.34      0.31       271\n",
            "weighted avg       0.49      0.57      0.53       271\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK-9C: Please apply a one-hot encoding method on the categorical data of the AD dataset."
      ],
      "metadata": {
        "id": "nI8zZHa9dlP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"NACCAPOE\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwbRZpVOAXMf",
        "outputId": "f8d41731-9edf-47b8-b594-b645e594ed2c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 1, 2, 4, 6, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.get_dummies(data[\"NACCAPOE\"])\n",
        "a.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8an5SGjRAg0y",
        "outputId": "ed28c8a6-f0e3-4286-abd8-d6477b285890"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   1  2  3  4  5  6\n",
              "0  0  0  1  0  0  0\n",
              "1  1  0  0  0  0  0\n",
              "2  1  0  0  0  0  0\n",
              "3  1  0  0  0  0  0\n",
              "4  1  0  0  0  0  0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fb99e8d-e0e8-47af-b6f8-b9485fe39e10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fb99e8d-e0e8-47af-b6f8-b9485fe39e10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fb99e8d-e0e8-47af-b6f8-b9485fe39e10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fb99e8d-e0e8-47af-b6f8-b9485fe39e10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(\"NACCAPOE\",axis=1)"
      ],
      "metadata": {
        "id": "JhC7tiueAmkQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"NACCAPOE-1\",\"NACCAPOE-2\",\"NACCAPOE-3\",\"NACCAPOE-4\",\"NACCAPOE-5\",\"NACCAPOE-6\"]\n",
        "\n",
        "for label,i in zip(labels,range(6)):\n",
        "  data[f\"{label}\"] = a.iloc[:,i]\n"
      ],
      "metadata": {
        "id": "nxYGMZ6MAqQB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[:,-6:].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RIufoB-IA0dp",
        "outputId": "8f3a20bb-a59e-4913-d3af-67f7a70cb4a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NACCAPOE-1  NACCAPOE-2  NACCAPOE-3  NACCAPOE-4  NACCAPOE-5  NACCAPOE-6\n",
              "0           0           0           1           0           0           0\n",
              "1           1           0           0           0           0           0\n",
              "2           1           0           0           0           0           0\n",
              "3           1           0           0           0           0           0\n",
              "4           1           0           0           0           0           0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76f3bd2c-ec14-4e6c-8dc0-c86caffc50ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NACCAPOE-1</th>\n",
              "      <th>NACCAPOE-2</th>\n",
              "      <th>NACCAPOE-3</th>\n",
              "      <th>NACCAPOE-4</th>\n",
              "      <th>NACCAPOE-5</th>\n",
              "      <th>NACCAPOE-6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76f3bd2c-ec14-4e6c-8dc0-c86caffc50ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76f3bd2c-ec14-4e6c-8dc0-c86caffc50ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76f3bd2c-ec14-4e6c-8dc0-c86caffc50ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK–9D: Please apply Multinomial Naive Bayes algorithms on the one-hot encoded categorical data of the AD dataset and predict the labels (CDRGLOB)."
      ],
      "metadata": {
        "id": "Itn5ITwedr9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()"
      ],
      "metadata": {
        "id": "_d5CZZ-uBUPI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data[\"CDRGLOB\"]\n",
        "X = data.drop(columns=\"CDRGLOB\")"
      ],
      "metadata": {
        "id": "PuRiLQ_EBZHf"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "OlssC4gMBqqn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_pred = gnb.fit(X_train, y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "SDJKGUQRB0JA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, clf_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUVgbwboB9I3",
        "outputId": "e1ca7eba-f10d-4bf4-e9e0-9c240d00df67"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.69      0.65       128\n",
            "           2       0.47      0.59      0.52       102\n",
            "           3       0.00      0.00      0.00        32\n",
            "           4       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.55       271\n",
            "   macro avg       0.27      0.32      0.29       271\n",
            "weighted avg       0.47      0.55      0.50       271\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK-9E: Please merge the normalized numeric values and one-hot encoded categorical values and then apply the following regression methods (Linear, Ridge and Lasso) to predict the labels as numeric values. Please do not forget to optimize them and use R2 for scoring."
      ],
      "metadata": {
        "id": "IhTx5w4Ad06s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = data.iloc[:,-6:].join(data[vol_colum])\n",
        "merged_data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "EAdEJoa3CGw4",
        "outputId": "887e841b-cab2-403f-8285-59baa76f4500"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   NACCAPOE-1  NACCAPOE-2  NACCAPOE-3  NACCAPOE-4  NACCAPOE-5  NACCAPOE-6  \\\n",
              "0           0           0           1           0           0           0   \n",
              "1           1           0           0           0           0           0   \n",
              "2           1           0           0           0           0           0   \n",
              "\n",
              "     NACCICV   NACCWMVL     CSFVOL    GRAYVOL   WHITEVOL    WMHVOL    CEREALL  \\\n",
              "0  70.019756  24.113158  13.999287  31.907311  24.092689  0.020460  59.518414   \n",
              "1  92.609508  28.044336  27.506229  37.058943  27.941057  0.103279  81.146134   \n",
              "2  71.076564  25.757106  13.009177  32.310282  25.689718  0.067387  60.894238   \n",
              "\n",
              "    CERETISS    CERECSF     CEREGR     CEREWH    LHIPPO    RHIPPO  LLATVENT  \\\n",
              "0  47.667882  11.850382  25.905772  21.741640  0.176984  0.172855  0.419091   \n",
              "1  57.250560  23.895575  31.311119  25.836162  0.187780  0.156679  1.496958   \n",
              "2  50.560046  10.334192  26.951522  23.541625  0.155284  0.159190  0.196791   \n",
              "\n",
              "   RLATVENT  THIRVENT   LFRCORT   RFRCORT   LOCCORT   ROCCORT  LPARCORT  \\\n",
              "0  0.340921  0.041194  4.499242  4.553199  1.440252  1.534467  2.599175   \n",
              "1  3.305512  0.118536  5.778925  5.494321  1.999856  1.937067  2.706964   \n",
              "2  0.190443  0.036624  5.144389  5.033542  1.555281  1.437597  2.554371   \n",
              "\n",
              "   RPARCORT  LTEMPCOR  RTEMPCOR      LCAC     LCACM      LCMF     LCMFM  \\\n",
              "0  2.545874  3.405822  3.132370  0.123773  0.098274  0.171354  0.087058   \n",
              "1  2.682905  3.736232  3.148834  0.179564  0.174870  0.318639  0.109147   \n",
              "2  2.745302  3.177460  3.195528  0.155772  0.114754  0.298848  0.091803   \n",
              "\n",
              "       LCUN     LCUNM      LENT     LENTM      LFUS     LFUSM   LINFPAR  \\\n",
              "0  0.202318  0.061043  0.252712  0.167946  0.537419  0.127082  0.617936   \n",
              "1  0.315705  0.082154  0.230617  0.186019  0.549843  0.121470  0.645493   \n",
              "2  0.203139  0.057133  0.238297  0.172375  0.500034  0.137705  0.571816   \n",
              "\n",
              "   LINFPARM  LINFTEMP  LINFTEMM   LINSULA  LINSULAM    LISTHC   LISTHCM  \\\n",
              "0  0.116231  0.626569  0.149383  0.307234  0.140084  0.121431  0.110536   \n",
              "1  0.103866  0.592093  0.148463  0.386709  0.200103  0.187780  0.123817   \n",
              "2  0.104499  0.525426  0.139658  0.319846  0.123543  0.150401  0.119149   \n",
              "\n",
              "    LLATOCC  LLATOCCM  LLATORBF  LLATORBM     LLING    LLINGM  LMEDORBF  \\\n",
              "0  0.561067  0.094896  0.401164  0.124874  0.313520  0.067830  0.228313   \n",
              "1  0.681876  0.083327  0.511700  0.141422  0.424265  0.075699  0.273454   \n",
              "2  0.592813  0.100593  0.458039  0.125497  0.316428  0.074712  0.287129   \n",
              "\n",
              "   LMEDORBM  LMIDTEMP  LMIDTEMM   LPARCEN  LPARCENM   LPARHIP  LPARHIPM  \\\n",
              "0  0.112928  0.718907  0.136966  0.155023  0.064957  0.225686  0.096983   \n",
              "1  0.140835  0.734689  0.109734  0.313944  0.117362  0.228857  0.103279   \n",
              "2  0.127450  0.631878  0.129403  0.213393  0.069829  0.184583  0.104011   \n",
              "\n",
              "    LPARSOP  LPARSOPM   LPARORB  LPARORBM   LPARTRI  LPARTRIM   LPERCAL  \\\n",
              "0  0.257776  0.098995  0.099750  0.103044  0.259093  0.091838  0.105946   \n",
              "1  0.351500  0.122644  0.092716  0.121470  0.247635  0.117949  0.197169   \n",
              "2  0.218276  0.081548  0.115242  0.111824  0.208510  0.100104  0.118660   \n",
              "\n",
              "   LPERCALM   LPOSCEN  LPOSCENM   LPOSCIN  LPOSCINM   LPRECEN  LPRECENM  \\\n",
              "0  0.062655  0.470515  0.060743  0.188996  0.094465  0.526158  0.074061   \n",
              "1  0.068070  0.515808  0.092129  0.268760  0.124404  0.782807  0.115602   \n",
              "2  0.056644  0.484408  0.064946  0.264178  0.108406  0.798394  0.080572   \n",
              "\n",
              "    LPRECUN  LPRECUNM   LROSANC  LROSANCM    LROSMF   LROSMFM    LSUPFR  \\\n",
              "0  0.505799  0.092754  0.183455  0.141916  0.591471  0.106522  1.404313   \n",
              "1  0.549843  0.104453  0.273454  0.194822  0.775766  0.116776  1.517496   \n",
              "2  0.489779  0.083990  0.185559  0.144053  0.602580  0.131845  1.450781   \n",
              "\n",
              "    LSUPFRM   LSUPPAR  LSUPPARM   LSUPTEM  LSUPTEMM   LSUPMAR  LSUPMARM  \\\n",
              "0  0.111382  0.607611  0.074907  0.755412  0.097799  0.612586  0.094290   \n",
              "1  0.146703  0.655469  0.107973  1.065651  0.137901  0.542801  0.121470   \n",
              "2  0.122567  0.545935  0.075689  0.832576  0.101081  0.617229  0.084967   \n",
              "\n",
              "     LTRTEM   LTRTEMM  \n",
              "0  0.045794  0.063681  \n",
              "1  0.075112  0.131446  \n",
              "2  0.052738  0.079107  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c28832a-2c88-4f04-afff-5cd97de03580\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NACCAPOE-1</th>\n",
              "      <th>NACCAPOE-2</th>\n",
              "      <th>NACCAPOE-3</th>\n",
              "      <th>NACCAPOE-4</th>\n",
              "      <th>NACCAPOE-5</th>\n",
              "      <th>NACCAPOE-6</th>\n",
              "      <th>NACCICV</th>\n",
              "      <th>NACCWMVL</th>\n",
              "      <th>CSFVOL</th>\n",
              "      <th>GRAYVOL</th>\n",
              "      <th>WHITEVOL</th>\n",
              "      <th>WMHVOL</th>\n",
              "      <th>CEREALL</th>\n",
              "      <th>CERETISS</th>\n",
              "      <th>CERECSF</th>\n",
              "      <th>CEREGR</th>\n",
              "      <th>CEREWH</th>\n",
              "      <th>LHIPPO</th>\n",
              "      <th>RHIPPO</th>\n",
              "      <th>LLATVENT</th>\n",
              "      <th>RLATVENT</th>\n",
              "      <th>THIRVENT</th>\n",
              "      <th>LFRCORT</th>\n",
              "      <th>RFRCORT</th>\n",
              "      <th>LOCCORT</th>\n",
              "      <th>ROCCORT</th>\n",
              "      <th>LPARCORT</th>\n",
              "      <th>RPARCORT</th>\n",
              "      <th>LTEMPCOR</th>\n",
              "      <th>RTEMPCOR</th>\n",
              "      <th>LCAC</th>\n",
              "      <th>LCACM</th>\n",
              "      <th>LCMF</th>\n",
              "      <th>LCMFM</th>\n",
              "      <th>LCUN</th>\n",
              "      <th>LCUNM</th>\n",
              "      <th>LENT</th>\n",
              "      <th>LENTM</th>\n",
              "      <th>LFUS</th>\n",
              "      <th>LFUSM</th>\n",
              "      <th>LINFPAR</th>\n",
              "      <th>LINFPARM</th>\n",
              "      <th>LINFTEMP</th>\n",
              "      <th>LINFTEMM</th>\n",
              "      <th>LINSULA</th>\n",
              "      <th>LINSULAM</th>\n",
              "      <th>LISTHC</th>\n",
              "      <th>LISTHCM</th>\n",
              "      <th>LLATOCC</th>\n",
              "      <th>LLATOCCM</th>\n",
              "      <th>LLATORBF</th>\n",
              "      <th>LLATORBM</th>\n",
              "      <th>LLING</th>\n",
              "      <th>LLINGM</th>\n",
              "      <th>LMEDORBF</th>\n",
              "      <th>LMEDORBM</th>\n",
              "      <th>LMIDTEMP</th>\n",
              "      <th>LMIDTEMM</th>\n",
              "      <th>LPARCEN</th>\n",
              "      <th>LPARCENM</th>\n",
              "      <th>LPARHIP</th>\n",
              "      <th>LPARHIPM</th>\n",
              "      <th>LPARSOP</th>\n",
              "      <th>LPARSOPM</th>\n",
              "      <th>LPARORB</th>\n",
              "      <th>LPARORBM</th>\n",
              "      <th>LPARTRI</th>\n",
              "      <th>LPARTRIM</th>\n",
              "      <th>LPERCAL</th>\n",
              "      <th>LPERCALM</th>\n",
              "      <th>LPOSCEN</th>\n",
              "      <th>LPOSCENM</th>\n",
              "      <th>LPOSCIN</th>\n",
              "      <th>LPOSCINM</th>\n",
              "      <th>LPRECEN</th>\n",
              "      <th>LPRECENM</th>\n",
              "      <th>LPRECUN</th>\n",
              "      <th>LPRECUNM</th>\n",
              "      <th>LROSANC</th>\n",
              "      <th>LROSANCM</th>\n",
              "      <th>LROSMF</th>\n",
              "      <th>LROSMFM</th>\n",
              "      <th>LSUPFR</th>\n",
              "      <th>LSUPFRM</th>\n",
              "      <th>LSUPPAR</th>\n",
              "      <th>LSUPPARM</th>\n",
              "      <th>LSUPTEM</th>\n",
              "      <th>LSUPTEMM</th>\n",
              "      <th>LSUPMAR</th>\n",
              "      <th>LSUPMARM</th>\n",
              "      <th>LTRTEM</th>\n",
              "      <th>LTRTEMM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>70.019756</td>\n",
              "      <td>24.113158</td>\n",
              "      <td>13.999287</td>\n",
              "      <td>31.907311</td>\n",
              "      <td>24.092689</td>\n",
              "      <td>0.020460</td>\n",
              "      <td>59.518414</td>\n",
              "      <td>47.667882</td>\n",
              "      <td>11.850382</td>\n",
              "      <td>25.905772</td>\n",
              "      <td>21.741640</td>\n",
              "      <td>0.176984</td>\n",
              "      <td>0.172855</td>\n",
              "      <td>0.419091</td>\n",
              "      <td>0.340921</td>\n",
              "      <td>0.041194</td>\n",
              "      <td>4.499242</td>\n",
              "      <td>4.553199</td>\n",
              "      <td>1.440252</td>\n",
              "      <td>1.534467</td>\n",
              "      <td>2.599175</td>\n",
              "      <td>2.545874</td>\n",
              "      <td>3.405822</td>\n",
              "      <td>3.132370</td>\n",
              "      <td>0.123773</td>\n",
              "      <td>0.098274</td>\n",
              "      <td>0.171354</td>\n",
              "      <td>0.087058</td>\n",
              "      <td>0.202318</td>\n",
              "      <td>0.061043</td>\n",
              "      <td>0.252712</td>\n",
              "      <td>0.167946</td>\n",
              "      <td>0.537419</td>\n",
              "      <td>0.127082</td>\n",
              "      <td>0.617936</td>\n",
              "      <td>0.116231</td>\n",
              "      <td>0.626569</td>\n",
              "      <td>0.149383</td>\n",
              "      <td>0.307234</td>\n",
              "      <td>0.140084</td>\n",
              "      <td>0.121431</td>\n",
              "      <td>0.110536</td>\n",
              "      <td>0.561067</td>\n",
              "      <td>0.094896</td>\n",
              "      <td>0.401164</td>\n",
              "      <td>0.124874</td>\n",
              "      <td>0.313520</td>\n",
              "      <td>0.067830</td>\n",
              "      <td>0.228313</td>\n",
              "      <td>0.112928</td>\n",
              "      <td>0.718907</td>\n",
              "      <td>0.136966</td>\n",
              "      <td>0.155023</td>\n",
              "      <td>0.064957</td>\n",
              "      <td>0.225686</td>\n",
              "      <td>0.096983</td>\n",
              "      <td>0.257776</td>\n",
              "      <td>0.098995</td>\n",
              "      <td>0.099750</td>\n",
              "      <td>0.103044</td>\n",
              "      <td>0.259093</td>\n",
              "      <td>0.091838</td>\n",
              "      <td>0.105946</td>\n",
              "      <td>0.062655</td>\n",
              "      <td>0.470515</td>\n",
              "      <td>0.060743</td>\n",
              "      <td>0.188996</td>\n",
              "      <td>0.094465</td>\n",
              "      <td>0.526158</td>\n",
              "      <td>0.074061</td>\n",
              "      <td>0.505799</td>\n",
              "      <td>0.092754</td>\n",
              "      <td>0.183455</td>\n",
              "      <td>0.141916</td>\n",
              "      <td>0.591471</td>\n",
              "      <td>0.106522</td>\n",
              "      <td>1.404313</td>\n",
              "      <td>0.111382</td>\n",
              "      <td>0.607611</td>\n",
              "      <td>0.074907</td>\n",
              "      <td>0.755412</td>\n",
              "      <td>0.097799</td>\n",
              "      <td>0.612586</td>\n",
              "      <td>0.094290</td>\n",
              "      <td>0.045794</td>\n",
              "      <td>0.063681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>92.609508</td>\n",
              "      <td>28.044336</td>\n",
              "      <td>27.506229</td>\n",
              "      <td>37.058943</td>\n",
              "      <td>27.941057</td>\n",
              "      <td>0.103279</td>\n",
              "      <td>81.146134</td>\n",
              "      <td>57.250560</td>\n",
              "      <td>23.895575</td>\n",
              "      <td>31.311119</td>\n",
              "      <td>25.836162</td>\n",
              "      <td>0.187780</td>\n",
              "      <td>0.156679</td>\n",
              "      <td>1.496958</td>\n",
              "      <td>3.305512</td>\n",
              "      <td>0.118536</td>\n",
              "      <td>5.778925</td>\n",
              "      <td>5.494321</td>\n",
              "      <td>1.999856</td>\n",
              "      <td>1.937067</td>\n",
              "      <td>2.706964</td>\n",
              "      <td>2.682905</td>\n",
              "      <td>3.736232</td>\n",
              "      <td>3.148834</td>\n",
              "      <td>0.179564</td>\n",
              "      <td>0.174870</td>\n",
              "      <td>0.318639</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.315705</td>\n",
              "      <td>0.082154</td>\n",
              "      <td>0.230617</td>\n",
              "      <td>0.186019</td>\n",
              "      <td>0.549843</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.645493</td>\n",
              "      <td>0.103866</td>\n",
              "      <td>0.592093</td>\n",
              "      <td>0.148463</td>\n",
              "      <td>0.386709</td>\n",
              "      <td>0.200103</td>\n",
              "      <td>0.187780</td>\n",
              "      <td>0.123817</td>\n",
              "      <td>0.681876</td>\n",
              "      <td>0.083327</td>\n",
              "      <td>0.511700</td>\n",
              "      <td>0.141422</td>\n",
              "      <td>0.424265</td>\n",
              "      <td>0.075699</td>\n",
              "      <td>0.273454</td>\n",
              "      <td>0.140835</td>\n",
              "      <td>0.734689</td>\n",
              "      <td>0.109734</td>\n",
              "      <td>0.313944</td>\n",
              "      <td>0.117362</td>\n",
              "      <td>0.228857</td>\n",
              "      <td>0.103279</td>\n",
              "      <td>0.351500</td>\n",
              "      <td>0.122644</td>\n",
              "      <td>0.092716</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.247635</td>\n",
              "      <td>0.117949</td>\n",
              "      <td>0.197169</td>\n",
              "      <td>0.068070</td>\n",
              "      <td>0.515808</td>\n",
              "      <td>0.092129</td>\n",
              "      <td>0.268760</td>\n",
              "      <td>0.124404</td>\n",
              "      <td>0.782807</td>\n",
              "      <td>0.115602</td>\n",
              "      <td>0.549843</td>\n",
              "      <td>0.104453</td>\n",
              "      <td>0.273454</td>\n",
              "      <td>0.194822</td>\n",
              "      <td>0.775766</td>\n",
              "      <td>0.116776</td>\n",
              "      <td>1.517496</td>\n",
              "      <td>0.146703</td>\n",
              "      <td>0.655469</td>\n",
              "      <td>0.107973</td>\n",
              "      <td>1.065651</td>\n",
              "      <td>0.137901</td>\n",
              "      <td>0.542801</td>\n",
              "      <td>0.121470</td>\n",
              "      <td>0.075112</td>\n",
              "      <td>0.131446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>71.076564</td>\n",
              "      <td>25.757106</td>\n",
              "      <td>13.009177</td>\n",
              "      <td>32.310282</td>\n",
              "      <td>25.689718</td>\n",
              "      <td>0.067387</td>\n",
              "      <td>60.894238</td>\n",
              "      <td>50.560046</td>\n",
              "      <td>10.334192</td>\n",
              "      <td>26.951522</td>\n",
              "      <td>23.541625</td>\n",
              "      <td>0.155284</td>\n",
              "      <td>0.159190</td>\n",
              "      <td>0.196791</td>\n",
              "      <td>0.190443</td>\n",
              "      <td>0.036624</td>\n",
              "      <td>5.144389</td>\n",
              "      <td>5.033542</td>\n",
              "      <td>1.555281</td>\n",
              "      <td>1.437597</td>\n",
              "      <td>2.554371</td>\n",
              "      <td>2.745302</td>\n",
              "      <td>3.177460</td>\n",
              "      <td>3.195528</td>\n",
              "      <td>0.155772</td>\n",
              "      <td>0.114754</td>\n",
              "      <td>0.298848</td>\n",
              "      <td>0.091803</td>\n",
              "      <td>0.203139</td>\n",
              "      <td>0.057133</td>\n",
              "      <td>0.238297</td>\n",
              "      <td>0.172375</td>\n",
              "      <td>0.500034</td>\n",
              "      <td>0.137705</td>\n",
              "      <td>0.571816</td>\n",
              "      <td>0.104499</td>\n",
              "      <td>0.525426</td>\n",
              "      <td>0.139658</td>\n",
              "      <td>0.319846</td>\n",
              "      <td>0.123543</td>\n",
              "      <td>0.150401</td>\n",
              "      <td>0.119149</td>\n",
              "      <td>0.592813</td>\n",
              "      <td>0.100593</td>\n",
              "      <td>0.458039</td>\n",
              "      <td>0.125497</td>\n",
              "      <td>0.316428</td>\n",
              "      <td>0.074712</td>\n",
              "      <td>0.287129</td>\n",
              "      <td>0.127450</td>\n",
              "      <td>0.631878</td>\n",
              "      <td>0.129403</td>\n",
              "      <td>0.213393</td>\n",
              "      <td>0.069829</td>\n",
              "      <td>0.184583</td>\n",
              "      <td>0.104011</td>\n",
              "      <td>0.218276</td>\n",
              "      <td>0.081548</td>\n",
              "      <td>0.115242</td>\n",
              "      <td>0.111824</td>\n",
              "      <td>0.208510</td>\n",
              "      <td>0.100104</td>\n",
              "      <td>0.118660</td>\n",
              "      <td>0.056644</td>\n",
              "      <td>0.484408</td>\n",
              "      <td>0.064946</td>\n",
              "      <td>0.264178</td>\n",
              "      <td>0.108406</td>\n",
              "      <td>0.798394</td>\n",
              "      <td>0.080572</td>\n",
              "      <td>0.489779</td>\n",
              "      <td>0.083990</td>\n",
              "      <td>0.185559</td>\n",
              "      <td>0.144053</td>\n",
              "      <td>0.602580</td>\n",
              "      <td>0.131845</td>\n",
              "      <td>1.450781</td>\n",
              "      <td>0.122567</td>\n",
              "      <td>0.545935</td>\n",
              "      <td>0.075689</td>\n",
              "      <td>0.832576</td>\n",
              "      <td>0.101081</td>\n",
              "      <td>0.617229</td>\n",
              "      <td>0.084967</td>\n",
              "      <td>0.052738</td>\n",
              "      <td>0.079107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c28832a-2c88-4f04-afff-5cd97de03580')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c28832a-2c88-4f04-afff-5cd97de03580 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c28832a-2c88-4f04-afff-5cd97de03580');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = data[\"CDRGLOB\"]\n",
        "X = merged_data"
      ],
      "metadata": {
        "id": "d44isITlEocx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "2T42SEB1EwSR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "lr_model = LinearRegression()\n",
        "lr_reg = lr_model.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "WVjfgAuSEGuz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "print(r2_score(y_test, lr_reg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNr7wP-TGftp",
        "outputId": "993117b3-c822-4d64-a3d8-636116b9b7e8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42804090905892045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rid_reg = Ridge()"
      ],
      "metadata": {
        "id": "MHxkMYGTHl5o"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_alpha = np.linspace(0.1,5.0, num=20)\n",
        "my_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4t2eR5OIrqB",
        "outputId": "b4f00ba5-027d-453e-d1e5-cd026c5a80f8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1       , 0.35789474, 0.61578947, 0.87368421, 1.13157895,\n",
              "       1.38947368, 1.64736842, 1.90526316, 2.16315789, 2.42105263,\n",
              "       2.67894737, 2.93684211, 3.19473684, 3.45263158, 3.71052632,\n",
              "       3.96842105, 4.22631579, 4.48421053, 4.74210526, 5.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'alpha': my_alpha}\n",
        "\n",
        "grid = GridSearchCV(estimator=rid_reg, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "id": "qMkl5LEyIJPB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "UTU7vfwAI-zo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MspGBvAzJFxH",
        "outputId": "baf7c961-93a6-4984-86d4-866166aa6d00"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.1}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rid_reg = grid.best_estimator_\n",
        "ridg_predictions = rid_reg.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "6Eo3DKsMJKTy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test, ridg_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jNeMPOeJUrY",
        "outputId": "c2c0ade0-24ea-43c3-f7c1-9bc9b2d37892"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.44032439538915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "las_reg = Lasso()\n",
        "grid = GridSearchCV(estimator=las_reg, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "id": "6EqcYdi7JbN4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzRo3xW9JsuA",
        "outputId": "07feb645-138c-4dca-9f71-886f883f2658"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.169e-01, tolerance: 5.804e-02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.814e-02, tolerance: 5.846e-02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uNES449J35Q",
        "outputId": "d939fe39-9a6d-454f-9f4f-03f6ad2c6c7a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.1}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "las_reg = grid.best_estimator_\n",
        "lasso_predictions = las_reg.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "WdvD3uibKAxH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test, lasso_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHJaZsG_KKcY",
        "outputId": "72f04061-76b2-4e90-cdca-6db60bfcc57a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28419006226396615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK-9F: Please find and apply the classification versions of the following algorithms (Linear, Ridge and Lasso) with the hyperparameter optimization."
      ],
      "metadata": {
        "id": "pWMQzraId5my"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tol = np.linspace(0.00001, 0.0009, num=4)\n",
        "my_C = np.linspace(0.1, 4.0, num=4)"
      ],
      "metadata": {
        "id": "iA-C3LhxMrAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear"
      ],
      "metadata": {
        "id": "PzsFYg_yd-Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lgr = LogisticRegression(penalty=\"none\", max_iter = 300)\n",
        "param_grid = {'tol': my_tol,\n",
        "              'C': my_C,}\n",
        "grid = GridSearchCV(estimator=model_lgr, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "id": "q73LXwv-Kew_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUP2HCFtJQiy",
        "outputId": "ef5ca97a-38c0-4ee7-f4a6-c2ee443d485a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=300, penalty='none'),\n",
              "             param_grid={'C': array([0.1, 1.4, 2.7, 4. ]),\n",
              "                         'tol': array([1.00000000e-05, 3.06666667e-04, 6.03333333e-04, 9.00000000e-04])})"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83ddUdtnQiUx",
        "outputId": "61c04473-0fa5-474b-c5f5-42469b0a8110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.1, 'tol': 1e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_class = grid.best_estimator_\n",
        "lin_reg_class_predictions = lin_reg_class.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nlSxpbwIsVU",
        "outputId": "24ec8a66-0285-44ab-825f-656156c43a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test, lin_reg_class_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6t6oiPoJ51_",
        "outputId": "f2e8cc42-6903-4868-edf5-b18aabad27ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2509355332539548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso (L1)"
      ],
      "metadata": {
        "id": "Nvqhnt0AeEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lgr_l1 = LogisticRegression(penalty = \"l1\",solver=\"liblinear\",max_iter=300)\n",
        "model_lgr_l1.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model_lgr_l1, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "id": "YKN8RBZdKE3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rjYlO14TOrE",
        "outputId": "2f53d17e-d62f-45ae-eb72-f63c827cb62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=LogisticRegression(max_iter=300, penalty='l1',\n",
              "                                          solver='liblinear'),\n",
              "             param_grid={'C': array([0.1, 1.4, 2.7, 4. ]),\n",
              "                         'tol': array([1.00000000e-05, 3.06666667e-04, 6.03333333e-04, 9.00000000e-04])})"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mjY0QZjTb0H",
        "outputId": "63901a6f-7781-4a73-dc57-9258aa6fb2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1.4000000000000001, 'tol': 0.0003066666666666667}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_l1_class = grid.best_estimator_\n",
        "lin_reg_l1_class_predictions = lin_reg_l1_class.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "47_4DuTcaDXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test, lin_reg_l1_class_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TENtxnxBaOTw",
        "outputId": "d370670f-55fd-4ae1-f762-e3ed30033d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.34312808300731423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge (L2)"
      ],
      "metadata": {
        "id": "lpGVznSFeLrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lgr_l2 = LogisticRegression(penalty = \"l2\",max_iter=300)\n",
        "model_lgr_l2.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model_lgr_l2, param_grid=param_grid, cv=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjHVEV7raSuO",
        "outputId": "96f61cd5-eea9-43be-f5e9-460413541879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UeBll_Habof",
        "outputId": "5e89703e-bb00-4be7-b161-1dad442859e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=300),\n",
              "             param_grid={'C': array([0.1, 1.4, 2.7, 4. ]),\n",
              "                         'tol': array([1.00000000e-05, 3.06666667e-04, 6.03333333e-04, 9.00000000e-04])})"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDZCmwTIaejG",
        "outputId": "51d792c8-52ed-4e98-b866-82c444486753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.1, 'tol': 1e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg_l2_class = grid.best_estimator_\n",
        "lin_reg_l2_class_predictions = lin_reg_l1_class.fit(X_train,y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "z9D9gb67aftJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(y_test, lin_reg_l2_class_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npCCyHckakyb",
        "outputId": "2c08b302-73c5-420c-e117-afa76aa45699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.34312808300731423\n"
          ]
        }
      ]
    }
  ]
}